{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c3f5d7-d100-4750-abd0-30ee107a5a62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5101e93-2e8f-4102-ad6c-6c9bd28cf19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",max_new_tokens=1000, return_full_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e9a2a9-91c3-4089-a8bb-01f821a9bd53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = \"LlamaIndex is a data framework for LLM-based applications which benefit from context augmentation. Such LLM systems have been termed as RAG systems, standing for “Retrieval-Augemented Generation”. .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf00bb02-930d-4d46-bddf-da56a655eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testdoc.txt', 'r') as file:\n",
    "    t1 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c79c2c6-3cfb-4541-93e7-bb108a9518a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '\\n\\nWhat is HBase?\\nHBase is a distributed, column-oriented database management system that is designed to handle large volumes of structured and semi-structured data. It is a NoSQL database that is designed to handle large volumes of data and is suitable for storing and querying data in a distributed environment.\\n\\nWhat is Oozie?\\nOozie is a workflow management system that is used to manage and orchestrate Hadoop jobs. It is a distributed system that allows users to define and execute workflows that can be executed on a cluster of Hadoop nodes. Oozie is used to manage the Hadoop jobs, monitor the job status, and manage the workflows.\\n\\nWhat is Sqoop?\\nSqoop is a tool used to transfer data between Hadoop and non-Hadoop systems. It is a command-line tool that allows users to transfer data between Hadoop and non-Hadoop systems. Sqoop is used to transfer data between Hadoop and non-Hadoop systems.\\n\\nWhat is Zookeeper?\\nZookeeper is a distributed coordination service that is used to manage the Hadoop cluster. It is used to manage the Hadoop cluster, monitor the cluster, and manage the Hadoop services. Zookeeper is used to manage the Hadoop cluster, monitor the cluster, and manage the Hadoop services.\\n\\nWhat is MapReduce?\\nMapReduce is a distributed computing framework that is used to process large datasets in Hadoop. It is a programming model that allows users to write MapReduce jobs that can be executed on a cluster of Hadoop nodes. MapReduce is used to process large datasets in Hadoop.\\n\\nWhat is Spark?\\nSpark is a distributed computing framework that is used to process large datasets in Hadoop. It is a programming model that allows users to write Spark jobs that can be executed on a cluster of Hadoop nodes. Spark is used to process large datasets in Hadoop.\\n\\nWhat is HBase?\\nHBase is a distributed, column-oriented database management system that is designed to handle large volumes of structured and semi-structured data. It is a NoSQL database that is designed to handle large volumes of data and is suitable for storing and querying data in a distributed environment.\\n\\nWhat is Oozie?\\nOozie is a workflow management system that is used to manage and orchestrate Hadoop jobs. It is a distributed system that allows users to define and execute workflows that can be executed on a cluster of Hadoop nodes. Oozie is used to manage the Hadoop jobs, monitor the job status, and manage the workflows.\\n\\nWhat is Sqoop?\\nSqoop is a tool used to transfer data between Hadoop and non-Hadoop systems. It is a command-line tool that allows users to transfer data between Hadoop and non-Hadoop systems. Sqoop is used to transfer data between Hadoop and non-Hadoop systems.\\n\\nWhat is Zookeeper?\\nZookeeper is a distributed coordination service that is used to manage the Hadoop cluster. It is used to manage the Hadoop cluster, monitor the cluster, and manage the Hadoop services. Zookeeper is used to manage the Hadoop cluster, monitor the cluster, and manage the Hadoop services.\\n\\nWhat is MapReduce?\\nMapReduce is a distributed computing framework that is used to process large datasets in Hadoop. It is a programming model that allows users to write MapReduce jobs that can be executed on a cluster of Hadoop nodes. MapReduce is used to process large datasets in Hadoop.\\n\\nWhat is Spark?\\nSpark is a distributed computing framework that is used to process large datasets in Hadoop. It is a programming model that allows users to write Spark jobs that can be executed on a cluster of Hadoop nodes. Spark is used to process large datasets in Hadoop.\\n\\nWhat is HBase?\\nHBase is a distributed, column-oriented database management system that is designed to handle large volumes of structured and semi-structured data. It is a NoSQL database that is designed to handle large volumes of data and is suitable for storing and querying data in a distributed environment.\\n\\nWhat is Oozie?\\nOozie is a workflow management system that is used to manage and orchestrate Hadoop jobs. It is a distributed system that allows users to define and execute workflows that can be executed on a cluster of Hadoop nodes. Oozie is used to manage the Hadoop jobs, monitor the job status, and manage the workflows.\\n\\nWhat is Sqoop?\\nSqoop is a tool used'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"You are an expert user extracting information to quiz people on documentation. You will be passed a page extracted from the documentation, write a numbered list of questions that can be answered based *solely* on the given text.: \"+ t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "890686e0-33b7-438b-ab72-8a35a3ffcdd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('testdoc2.txt', 'r') as file:\n",
    "    t1 = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b4942f8-02cb-4542-b498-9ca5c5ba9b05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an expert user extracting information to quiz people on documentation. You will be passed a page extracted from the documentation.  \"\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Write a numbered list of 5 questions that can be answered based *solely* on the given text, and include the answer under each question followed by the characters ###$$$: \"+ t1},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1026241c-26f8-45f5-8bb5-ad628c5a61a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an expert user extracting information to quiz people on documentation. You will be passed a page extracted from the documentation.  \"\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Write a numbered list of 3 questions that can be answered based *solely* on the given text, and include the answer under each question.  Prefix the question with the word question followed by a colon, and prefix the answer with the word answer followed by a colon\"+ t1},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb7f529-91a4-48e4-8168-e5a8e83a9549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "891c0019-6c54-40d0-9f7c-461986598171",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is the purpose of LLM-based applications and how do they benefit from context augmentation?\n",
      "   Answer: LLM-based applications are data frameworks for LLM-based applications which benefit from context augmentation. These applications use language models (LMs) to generate natural language output, and context augmentation refers to the process of adding context to the input to enhance the model's ability to generate accurate and relevant output.\n",
      "\n",
      "2. What is context augmentation and how does it benefit LLM-based applications?\n",
      "   Answer: Context augmentation is the process of adding context to the input to enhance the model's ability to generate accurate and relevant output. By adding context, the LLM-based application can better understand the context of the input and generate more relevant and accurate output.\n",
      "\n",
      "3. What is the LlamaIndex and how does it benefit LLM-based applications?\n",
      "   Answer: LlamaIndex is a data framework for LLM-based applications which benefit from context augmentation. It is a tool for generating context-aware queries and responses for LLM-based applications. By using LlamaIndex, LLM-based applications can generate more accurate and relevant output by leveraging contextual information.\n",
      "\n",
      "1. Question: What is the purpose of LLM-based applications and how do they benefit from context augmentation?\n",
      "   Answer: The purpose of LLM-based applications is to generate natural language output, and context augmentation is a process that enhances the model's ability to generate accurate and relevant output by adding context to the input.\n",
      "\n",
      "2. Question: What is context augmentation and how does it benefit LLM-based applications?\n",
      "   Answer: Context augmentation is the process of adding context to the input to enhance the model's ability to generate accurate and relevant output. By adding context, the LLM-based application can better understand the context of the input and generate more relevant and accurate output.\n",
      "\n",
      "3. Question: What is LlamaIndex and how does it benefit LLM-based applications?\n",
      "   Answer: LlamaIndex is a data framework for LLM-based applications which benefit from context augmentation. It is a tool for generating context-aware queries and responses for LLM-based applications. By using LlamaIndex, LLM-based applications can generate more accurate and relevant output by leveraging contextual information.\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(prompt, max_new_tokens=1000, do_sample=True, temperature=0.01, top_k=3, top_p=0.05)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "707c5fb2-796d-4eb8-9963-95311db7a8c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an expert user extracting information to quiz people on documentation. You will be passed a text extracted from the documentation.  \"\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Write a question that can be answered based *solely* on the given text, and include the answer under each question, prefixing the question with the word question followed by a colon, and prefixing the answer with the word answer followed by a colon\"+ t1},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70107132-5eee-4b35-82fd-e2750936c8a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an expert user extracting information to quiz people on documentation. You will be passed a text extracted from the documentation.  \"\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Write three questions that can be answered based *solely* on the given text, and include the answer under each question, prefixing each question with the word question followed by a colon, and prefixing each answer with the word answer followed by a colon\"+ t1},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ce5cd04-2f36-4d52-a8bf-33b6bdfd3bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab98bb53-15a1-482f-8647-6d18c91c73fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: Can you provide a brief overview of LLM-based applications and how they benefit from context augmentation?\n",
      "\n",
      "Answer: LLM-based applications are data frameworks for LLM-based applications that benefit from context augmentation. Context augmentation refers to the process of adding context to the input text to enhance the model's ability to generate accurate and relevant responses.\n",
      "\n",
      "Question 2: Can you explain how LLM-based applications use context augmentation to benefit from the model's ability to generate accurate and relevant responses?\n",
      "\n",
      "Answer: LLM-based applications use context augmentation to enhance the model's ability to generate accurate and relevant responses by providing additional context to the input text. This context can be in the form of relevant information, such as previous questions or contextual information, which the model can use to generate more accurate and relevant responses.\n",
      "\n",
      "Question 3: Can you provide an example of how LLM-based applications use context augmentation to generate accurate and relevant responses?\n",
      "\n",
      "Answer: Yes, an example of how LLM-based applications use context augmentation to generate accurate and relevant responses is in the context of a quiz application. The application may ask a question based on the given text, and the model may use contextual information to generate a response that is more accurate and relevant to the question. For example, if the given text mentions a specific product, the model may use information about that product to generate a response that is more relevant to the question.\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(prompt, max_new_tokens=1000, do_sample=True, temperature=0.0001, top_k=1, top_p=0.0001)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f48f5-5c81-442d-84f8-5297163597f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an expert user extracting information to quiz people on documentation. You will be passed a page extracted from the documentation.  \"\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Write a numbered list of 3 questions that can be answered based *solely* on the given text, and include the answer under each question, prefixing the question with the word question followed by a colon, and prefixing the answer with the word answer followed by a colon\"+ t1},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5678b0-c65d-4c4e-b0e0-8e81347c726e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "176eed18-9211-432e-ba78-4ceee5b5b6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is Retrieval-Augmented Generation (RAG), and how does it work?\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a technique for enriching prompts with additional information from a user's data sources. By adding retrieved documents as context, RAG overcomes the limitations of fine-tuning a LLM with existing data. This technique involves two key steps:\n",
      "\n",
      "1. Retrieving information from user data sources first.\n",
      "2. Adding this information to the user's prompt as context.\n",
      "\n",
      "RAG overcomes the weaknesses of fine-tuning by:\n",
      "\n",
      "1. Fetching information only when needed, rather than training a LLM with existing data.\n",
      "2. Enhancing the prompt with the retrieved information, making it more relevant and accurate.\n",
      "\n",
      "RAG is useful for context-sensitive questions or queries where the LLM's output needs to be tailored to the user's context.\n",
      "\n",
      "2. How can LlamaIndex provide tools for beginners, advanced users, and everyone in between?\n",
      "\n",
      "LlamaIndex provides a high-level API for beginners and advanced users to ingest and query their data in 5 lines of code. For more complex applications, our lower-level APIs allow advanced users to customize and extend any module—data connectors, indices, retrievers, query engines, and more—to fit their needs.\n",
      "\n",
      "3. Why is LlamaIndex suitable for context augmentation, and how does it differ from fine-tuning?\n",
      "\n",
      "RAG works by adding information from a user's data sources as context to the LLM's prompt, allowing the LLM to produce more accurate and relevant output. Fine-tuning is a more expensive process that requires training a LLM with specific data, which can be costly. RAG avoids this by relying on readily available data and using context to augment the LLM's output.\n",
      "\n",
      "In contrast, fine-tuning involves training a LLM with specific data, which can be time-consuming and expensive. Fine-tuning is also limited in terms of context, which can limit the ability of the LLM to generate accurate and relevant output.\n",
      "\n",
      "4. Who is LlamaIndex suitable for, and what is the difference between LlamaIndex and other context augmentation techniques?\n",
      "\n",
      "LlamaIndex is suitable for beginners and advanced users alike, and it differs from other context augmentation techniques by providing a higher-level API for ingesting and querying data. LlamaIndex's lower-level APIs allow advanced users to customize and extend any module—data connectors, indices, retrievers, query engines, and more—to fit their needs.\n",
      "\n",
      "In summary, LlamaIndex provides a high-level API for beginners and advanced users to ingest and query their data in 5 lines of code. It differs from fine-tuning by providing a more cost-effective and context-sensitive approach for context augmentation.\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(prompt, max_new_tokens=1000, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e035454-be76-429c-9438-48f785f87d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Explain the difference between LLMs and LlamaIndex and their use cases.\n",
      "\n",
      "LLMs are trained on vast amounts of publicly available data like Wikipedia, mailing lists, textbooks, source code, and more. While LLMs are useful for generating natural language responses to questions, they are not designed for handling private or domain-specific data. LlamaIndex provides a way to ingest and augment private or domain-specific data for use with LLMs. LlamaIndex's use cases include:\n",
      "\n",
      "1. Data integration: LlamaIndex enables the seamless integration of private data into LLM-based applications.\n",
      "\n",
      "2. Context augmentation: LlamaIndex allows for contextual enrichment of queries by retrieving relevant information from the user's data sources.\n",
      "\n",
      "3. Machine learning: LlamaIndex can be used for fine-tuning LLMs for specific applications, such as chatbots or semi-autonomous agents.\n",
      "\n",
      "4. Data augmentation: LlamaIndex enables the creation of new, private datasets for use with LLMs.\n",
      "\n",
      "5. Natural language processing: LlamaIndex provides natural language access to private data through query engines, chat engines, and data agents.\n",
      "\n",
      "6. Application integrations: LlamaIndex allows for the integration of LlamaIndex modules into other ecosystems, such as LangChain, Flask, Docker, and ChatGPT.\n",
      "\n",
      "Overall, LlamaIndex provides a powerful way to ingest and augment private or domain-specific data for use with LLMs.\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(prompt, max_new_tokens=1000, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcfdf9d-2ffc-470d-8bd2-0f5405a12066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
