{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29791da6-41dc-465e-9730-a0f5ab91b1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "797eccae-2a39-4a31-a8fe-e204f020e0fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"documents.pkl\"\n",
    "filepath = \"\"\n",
    "fileuri = filepath+filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d1bd29-75b9-404f-ba5a-775217a72952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(fileuri, 'rb') as documents: \n",
    "    docs = pickle.load(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd22659-b82f-4877-8d10-cdb6a9694a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378d93397ed249eab27ded634ad777da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3291cc147c1a4e90a1f6fa70ad818d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3edb2e4dda944b88209a50ea57fb450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75604c4e2f5b4d1f8950ba56db0562a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c4297418354a598f2496d6d9db0487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ea9909377940148b19cf52836aff09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "\n",
    "text_splitter = SentenceSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "generate_kwargs={\"temperature\": 0.7, \n",
    "                 \"do_sample\": True, \n",
    "                 \"top_k\": 50, \n",
    "                 \"top_p\": 0.70\n",
    "                }\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=\t2048,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs= generate_kwargs,\n",
    "    tokenizer_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    device_map=\"auto\",\n",
    "    tokenizer_kwargs={\"max_length\": 2048},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    # model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm = llm, \n",
    "                                               text_splitter = text_splitter, \n",
    "                                               embed_model=\"local\")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs, service_context=service_context\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b3059e-2471-4bf2-86d6-9863a10ac40d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.indices.vector_store.base.VectorStoreIndex at 0x7fca68181be0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19510cae-215a-48b9-aea8-2e62b2bde50b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import pymilvus\n",
    "from milvus import default_server\n",
    "from llama_index.vector_stores import MilvusVectorStore\n",
    "from pymilvus import connections, Collection, utility\n",
    "\n",
    "# Start Milvus Vector DB\n",
    "default_server.stop()\n",
    "default_server.set_base_dir('milvus-data')\n",
    "default_server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ffcbdb-b548-4bed-bf44-93cae10bdb4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import milvus\n",
    "from llama_index.vector_stores import MilvusVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "# vector_store = MilvusVectorStore(dim=2048, overwrite=True)\n",
    "\n",
    "# vector_store = MilvusVectorStore(dim=2048, overwrite=True)\n",
    "vector_store = MilvusVectorStore(dim=384, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b221fe2-0daf-4bf5-820b-ffd7080c5120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aafef15-a502-48e3-8658-94da9deab74d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    docs, storage_context=storage_context, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10069eea-f539-4edd-83f3-4e2168ddddfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: \n",
      "LlamaIndex is a data framework for LLM-based applications to ingest, structure, and access private or domain-specific data. It’s available in Python (these docs) and Typescript. It’s for beginners, advanced users, and everyone in between.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"What is LlamaIndex?\")\n",
    "print(\"Res:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1885c-49b6-4f28-b558-88f49d61134e",
   "metadata": {
    "tags": []
   },
   "source": [
    "LlamaIndex is a data framework for LLM-based applications to ingest, structure, and access private or domain-specific data. It’s available in Python (these docs) and Typescript. LlamaIndex is designed for beginners, advanced users, and everyone in between. Its high-level API allows for easy ingestion and querying of data in 5 lines of code. Its lower-level APIs allow for customization and extension to fit complex applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb1c5739-fb74-4df4-b124-87f9a39386aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='398cc74a-580d-41c2-a85f-0b980e4ff0ef', embedding=None, metadata={'url': 'https://gpt-index.readthedocs.io/en/latest'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='767b9945a76c081d709c631a4c4bd5ea20b3b2ca30c6b52133fa78638aab3be3', text='LlamaIndex is a data framework for LLM-based applications to ingest, structure, and access private or domain-specific data. It’s available in Python (these docs) and Typescript.\\n🚀 Why LlamaIndex?\\uf0c1\\nLLMs offer a natural language interface between humans and data. Widely available models come pre-trained on huge amounts of publicly available data like Wikipedia, mailing lists, textbooks, source code and more.\\nHowever, while LLMs are trained on a great deal of data, they are not trained on your data, which may be private or specific to the problem you’re trying to solve. It’s behind APIs, in SQL databases, or trapped in PDFs and slide decks.\\nYou may choose to fine-tune a LLM with your data, but:\\nTraining a LLM is expensive.\\nDue to the cost to train, it’s hard to update a LLM with latest information.\\nObservability is lacking. When you ask a LLM a question, it’s not obvious how the LLM arrived at its answer.\\nLlamaIndex takes a different approach called Retrieval-Augmented Generation (RAG). Instead of asking LLM to generate an answer immediately, LlamaIndex:\\nretrieves information from your data sources first,\\nadds it to your question as context, and\\nasks the LLM to answer based on the enriched prompt.\\nRAG overcomes all three weaknesses of the fine-tuning approach:\\nThere’s no training involved, so it’s cheap.\\nData is fetched only when you ask for them, so it’s always up to date.\\nLlamaIndex can show you the retrieved documents, so it’s more trustworthy.\\nLlamaIndex imposes no restriction on how you use LLMs. You can still use LLMs as auto-complete, chatbots, semi-autonomous agents, and more (see Use Cases on the left). It only makes LLMs more relevant to you.\\n🦙 How can LlamaIndex help?\\uf0c1\\nLlamaIndex provides the following tools:\\nData connectors ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more.\\nData indexes structure your data in intermediate representations that are easy and performant for LLMs to consume.\\nEngines provide natural language access to your data. For example:\\nQuery engines are powerful retrieval interfaces for knowledge-augmented output.\\nChat engines are conversational interfaces for multi-message, “back and forth” interactions with your data.\\nData agents are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more.\\nApplication integrations tie LlamaIndex back into the rest of your ecosystem. This could be LangChain, Flask, Docker, ChatGPT, or… anything else!\\n👨\\u200d👩\\u200d👧\\u200d👦 Who is LlamaIndex for?\\uf0c1\\nLlamaIndex provides tools for beginners, advanced users, and everyone in between.\\nOur high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code.\\nFor more complex applications, our lower-level APIs allow advanced users to customize and extend any module—data connectors, indices, retrievers, query engines, reranking modules—to fit their needs.\\nGetting Started\\uf0c1\\nTo install the library:\\npip install llama-index\\nWe recommend starting at how to read these docs, which will point you to the right place based on your experience level.\\n🗺️ Ecosystem\\uf0c1\\nTo download or contribute, find LlamaIndex on:\\nGithub: https://github.com/jerryjliu/llama_index\\nPyPi:\\nLlamaIndex: https://pypi.org/project/llama-index/.\\nGPT Index (duplicate): https://pypi.org/project/gpt-index/.\\nNPM (Typescript/Javascript):\\nGithub: https://github.com/run-llama/LlamaIndexTS\\nDocs: https://ts.llamaindex.ai/\\nLlamaIndex.TS: https://www.npmjs.com/package/llamaindex\\nAssociated projects\\uf0c1\\n🏡 LlamaHub: https://llamahub.ai | A large (and growing!) collection of custom data connectors\\n🧪 LlamaLab: https://github.com/run-llama/llama-lab | Ambitious projects built on top of LlamaIndex', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1032f2b-1d98-4af2-930d-97a5ae667815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "connection = connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddc64051-2318-4fab-b90f-b38306f106b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['llamalection', 'book']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c85b6668-a448-4636-afef-9434fee2eecb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auto_id': False, 'description': '', 'fields': [{'name': 'id', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 65535}, 'is_primary': True, 'auto_id': False}, {'name': 'embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 384}}], 'enable_dynamic_field': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = Collection(\"llamalection\")\n",
    "collection.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c03c5667-ebb9-4595-bdd3-ef2c53d8aff1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.num_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a0db5ca-388a-456c-b49c-8539426526d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = collection.query(\n",
    "  expr = \"\",\n",
    "  offset = 60,\n",
    "  limit = 10, \n",
    "  output_fields = ['id', 'embedding']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57aabc4a-cca8-4dbe-b6d8-d2b9246a7468",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'd78b8eaa-8b81-4c8d-b21d-1a7fb39ac2ca',\n",
       " 'embedding': [-0.06251519,\n",
       "  -0.013404677,\n",
       "  -0.022419333,\n",
       "  -0.0057119303,\n",
       "  -0.0048313807,\n",
       "  0.018648615,\n",
       "  0.010658559,\n",
       "  -0.0039329743,\n",
       "  0.008745981,\n",
       "  -0.025675053,\n",
       "  -0.013916663,\n",
       "  -0.030394591,\n",
       "  0.04816568,\n",
       "  0.0051902444,\n",
       "  -0.016413933,\n",
       "  0.011856366,\n",
       "  0.010102913,\n",
       "  0.009473617,\n",
       "  0.028741565,\n",
       "  0.055245023,\n",
       "  0.036948513,\n",
       "  0.013249658,\n",
       "  -0.057163365,\n",
       "  0.0006234221,\n",
       "  0.012859257,\n",
       "  -0.003343579,\n",
       "  -0.025322476,\n",
       "  -0.015072759,\n",
       "  -0.02962951,\n",
       "  -0.20529802,\n",
       "  -0.024722375,\n",
       "  -0.0052649616,\n",
       "  0.009839829,\n",
       "  0.0047894204,\n",
       "  0.007153571,\n",
       "  0.014824283,\n",
       "  0.015458113,\n",
       "  -0.0195616,\n",
       "  -0.027394686,\n",
       "  0.006797099,\n",
       "  -0.018606357,\n",
       "  -0.009030563,\n",
       "  0.03189251,\n",
       "  0.01812224,\n",
       "  0.034556318,\n",
       "  -0.021895714,\n",
       "  -0.029221345,\n",
       "  -0.030029615,\n",
       "  -0.062034365,\n",
       "  -0.013681108,\n",
       "  -0.03382084,\n",
       "  0.0060293297,\n",
       "  0.002489617,\n",
       "  0.031486444,\n",
       "  0.012832193,\n",
       "  0.022034839,\n",
       "  0.049162697,\n",
       "  0.05765955,\n",
       "  0.02551283,\n",
       "  0.00721585,\n",
       "  0.012845799,\n",
       "  0.017433006,\n",
       "  -0.20216735,\n",
       "  0.02285326,\n",
       "  -0.00030834827,\n",
       "  0.034929316,\n",
       "  -0.0045542405,\n",
       "  -0.08286025,\n",
       "  -0.0028378975,\n",
       "  0.069985524,\n",
       "  -0.007590336,\n",
       "  0.008237852,\n",
       "  0.006627904,\n",
       "  0.0011869146,\n",
       "  0.00084363087,\n",
       "  -0.004560584,\n",
       "  -0.022715727,\n",
       "  0.024650352,\n",
       "  0.02848677,\n",
       "  0.036189776,\n",
       "  0.0005183718,\n",
       "  -0.05722601,\n",
       "  -0.011058782,\n",
       "  0.006993227,\n",
       "  -0.044784464,\n",
       "  -0.011155575,\n",
       "  0.034435403,\n",
       "  -0.027105885,\n",
       "  0.025828598,\n",
       "  -0.041544974,\n",
       "  0.002324411,\n",
       "  -0.02068017,\n",
       "  -0.010314229,\n",
       "  0.023878824,\n",
       "  -0.014086253,\n",
       "  0.003956778,\n",
       "  -0.011702358,\n",
       "  0.00055030675,\n",
       "  0.019447712,\n",
       "  0.5919925,\n",
       "  -0.015149675,\n",
       "  0.029781174,\n",
       "  0.0040336787,\n",
       "  -0.0041743144,\n",
       "  0.007112609,\n",
       "  -0.040051,\n",
       "  -0.0034749708,\n",
       "  -0.039159805,\n",
       "  0.019687098,\n",
       "  -0.0005223983,\n",
       "  0.005572584,\n",
       "  -0.022559453,\n",
       "  0.085134506,\n",
       "  -0.016680932,\n",
       "  0.0023804524,\n",
       "  -0.018445192,\n",
       "  0.021086983,\n",
       "  0.018370882,\n",
       "  -0.035177134,\n",
       "  -0.037782215,\n",
       "  -0.029008595,\n",
       "  0.06352385,\n",
       "  0.014350276,\n",
       "  0.008953738,\n",
       "  -0.01278182,\n",
       "  -0.011985944,\n",
       "  0.032850932,\n",
       "  0.084998086,\n",
       "  -0.018278394,\n",
       "  0.029743828,\n",
       "  0.037961267,\n",
       "  0.0036597624,\n",
       "  -0.037191138,\n",
       "  -0.02335406,\n",
       "  0.04381499,\n",
       "  0.0071345437,\n",
       "  0.009415072,\n",
       "  -0.001711517,\n",
       "  -0.003380161,\n",
       "  -0.012704581,\n",
       "  -0.06327488,\n",
       "  -0.015820716,\n",
       "  -0.017235631,\n",
       "  0.014470854,\n",
       "  -0.04675928,\n",
       "  0.018928407,\n",
       "  -0.01912721,\n",
       "  -0.037908707,\n",
       "  -0.054030877,\n",
       "  0.028484369,\n",
       "  -0.019254558,\n",
       "  0.035925828,\n",
       "  0.011662478,\n",
       "  -0.029251112,\n",
       "  0.014630586,\n",
       "  0.038484108,\n",
       "  0.091653526,\n",
       "  0.027873693,\n",
       "  -0.0741478,\n",
       "  -0.00514368,\n",
       "  -0.013093584,\n",
       "  -0.019864796,\n",
       "  -0.035998452,\n",
       "  0.051180705,\n",
       "  0.007923181,\n",
       "  -0.08741071,\n",
       "  -0.020462247,\n",
       "  0.02186664,\n",
       "  0.043612737,\n",
       "  -0.019324284,\n",
       "  0.03003611,\n",
       "  -0.018102322,\n",
       "  -0.0529741,\n",
       "  0.0059084343,\n",
       "  0.014523369,\n",
       "  0.028272927,\n",
       "  -0.0006082805,\n",
       "  0.0050076125,\n",
       "  0.022167355,\n",
       "  -0.022021325,\n",
       "  -0.010305446,\n",
       "  -0.038269922,\n",
       "  -0.014274574,\n",
       "  0.025750892,\n",
       "  -0.02593716,\n",
       "  -0.024081642,\n",
       "  -0.006423548,\n",
       "  0.0015935582,\n",
       "  0.042602006,\n",
       "  0.005774303,\n",
       "  -0.056949023,\n",
       "  0.012793234,\n",
       "  -0.016431024,\n",
       "  0.015252673,\n",
       "  -0.016003247,\n",
       "  -0.03702364,\n",
       "  -0.0033605976,\n",
       "  0.031612895,\n",
       "  -0.026970679,\n",
       "  0.006824327,\n",
       "  0.012092257,\n",
       "  0.03582623,\n",
       "  -0.014964272,\n",
       "  -0.012752679,\n",
       "  -0.023728848,\n",
       "  -0.0070603862,\n",
       "  0.019061837,\n",
       "  -0.032238655,\n",
       "  0.029252313,\n",
       "  0.010882314,\n",
       "  -0.020609153,\n",
       "  0.031824563,\n",
       "  0.014363733,\n",
       "  -0.01954142,\n",
       "  -0.015197247,\n",
       "  0.013254117,\n",
       "  0.03838501,\n",
       "  0.0010102615,\n",
       "  0.020463776,\n",
       "  -0.030658767,\n",
       "  -0.023405071,\n",
       "  -0.05318454,\n",
       "  -0.05466179,\n",
       "  -0.23040442,\n",
       "  -0.04757095,\n",
       "  -0.035989664,\n",
       "  -0.00950497,\n",
       "  0.048843823,\n",
       "  -0.07751161,\n",
       "  0.0010603754,\n",
       "  -0.013810384,\n",
       "  0.01042631,\n",
       "  0.07370339,\n",
       "  0.07930891,\n",
       "  0.01664579,\n",
       "  -0.007134683,\n",
       "  0.021398941,\n",
       "  -0.012948095,\n",
       "  0.023033695,\n",
       "  0.0023010732,\n",
       "  -0.013258685,\n",
       "  -0.037153713,\n",
       "  0.022330636,\n",
       "  -0.022271644,\n",
       "  0.0013862082,\n",
       "  -0.00331791,\n",
       "  -0.095804796,\n",
       "  0.0074751596,\n",
       "  0.003889455,\n",
       "  0.16857888,\n",
       "  0.019688519,\n",
       "  0.070387654,\n",
       "  -0.013603762,\n",
       "  0.011504092,\n",
       "  -0.0016786099,\n",
       "  -0.03448944,\n",
       "  -0.059158444,\n",
       "  0.029127996,\n",
       "  -0.029392581,\n",
       "  0.04031497,\n",
       "  -0.006454813,\n",
       "  -0.021469628,\n",
       "  -0.00082184473,\n",
       "  -0.04581743,\n",
       "  -0.0049941144,\n",
       "  -0.018159177,\n",
       "  -0.074120015,\n",
       "  -0.018017367,\n",
       "  0.02001013,\n",
       "  -0.0320258,\n",
       "  0.01897064,\n",
       "  0.0028620642,\n",
       "  0.061044574,\n",
       "  -0.031198455,\n",
       "  0.012339432,\n",
       "  0.009118499,\n",
       "  0.018711,\n",
       "  -0.01219947,\n",
       "  -0.0069330726,\n",
       "  -0.080653004,\n",
       "  -0.023415454,\n",
       "  -0.024976771,\n",
       "  0.024863103,\n",
       "  -0.012600897,\n",
       "  -0.016062448,\n",
       "  -0.024608444,\n",
       "  -0.017610354,\n",
       "  0.0142005,\n",
       "  -0.033498935,\n",
       "  0.028720833,\n",
       "  -0.010915824,\n",
       "  0.051640224,\n",
       "  -0.021166738,\n",
       "  -0.009812415,\n",
       "  0.05594219,\n",
       "  -0.012154024,\n",
       "  -0.01493245,\n",
       "  -0.0042918976,\n",
       "  0.031505834,\n",
       "  0.033930168,\n",
       "  -0.029827964,\n",
       "  -0.027693039,\n",
       "  -0.025813257,\n",
       "  0.05653196,\n",
       "  -0.052386787,\n",
       "  0.043219272,\n",
       "  0.02586513,\n",
       "  0.027810058,\n",
       "  0.022592863,\n",
       "  0.06433918,\n",
       "  -0.012073516,\n",
       "  0.031903975,\n",
       "  -0.0040057823,\n",
       "  -0.07017596,\n",
       "  0.020230528,\n",
       "  -0.022894496,\n",
       "  -0.043599408,\n",
       "  0.030252747,\n",
       "  -0.0065771486,\n",
       "  -0.298687,\n",
       "  -0.0123444665,\n",
       "  -0.009515036,\n",
       "  0.058718707,\n",
       "  0.021200005,\n",
       "  -0.0027906857,\n",
       "  0.022373233,\n",
       "  -0.009244309,\n",
       "  0.0074323886,\n",
       "  0.0081976205,\n",
       "  0.0020453809,\n",
       "  0.0048254943,\n",
       "  0.01356673,\n",
       "  0.011641287,\n",
       "  0.04997425,\n",
       "  0.008335774,\n",
       "  0.047070496,\n",
       "  -0.036855586,\n",
       "  -0.004054202,\n",
       "  0.0103567885,\n",
       "  0.03730985,\n",
       "  0.027698467,\n",
       "  0.19848393,\n",
       "  -0.0045440914,\n",
       "  0.019493988,\n",
       "  0.010532802,\n",
       "  0.0018044047,\n",
       "  -0.0002035611,\n",
       "  -0.03863186,\n",
       "  0.033196833,\n",
       "  -0.015162907,\n",
       "  0.02075491,\n",
       "  0.104650915,\n",
       "  -0.044617873,\n",
       "  -0.014514246,\n",
       "  0.009244995,\n",
       "  -0.023404673,\n",
       "  0.03379556,\n",
       "  0.01142072,\n",
       "  -0.012124973,\n",
       "  -0.052138343,\n",
       "  0.007269199,\n",
       "  -0.010607025,\n",
       "  0.039641894,\n",
       "  0.043622613,\n",
       "  0.022958154,\n",
       "  -0.013316294,\n",
       "  -0.010041795,\n",
       "  0.0038854766,\n",
       "  0.022275668,\n",
       "  -0.01693035,\n",
       "  0.045136422,\n",
       "  -0.004399182,\n",
       "  -0.027942516,\n",
       "  0.008578744,\n",
       "  0.028995184,\n",
       "  -0.003952432,\n",
       "  -0.015116093,\n",
       "  0.021633415,\n",
       "  0.019140353,\n",
       "  -0.005438845,\n",
       "  -0.037159365,\n",
       "  0.054627102,\n",
       "  0.04480745,\n",
       "  0.008688669]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a90fd8e-1b82-494f-af2d-2a618ba2692b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res[0]['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62d5274b-d640-4526-9e90-8ac7974009b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = collection.query(\n",
    "  expr = \"\",\n",
    "  offset = 60,\n",
    "  limit = 10, \n",
    "  output_fields = ['id', 'embedding']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29acec2-0be8-432f-8d5e-1c2c6d6bb383",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "1) Check embedding size of Lllamaindex Huggingface embettings in (are they ~300 or 2k)\n",
    "2) See if fields can be inserted to Milvus via Llamaindex. i.e. more than just embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "443fe202-aa8f-4745-be53-c6fe01051a27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='398cc74a-580d-41c2-a85f-0b980e4ff0ef', embedding=None, metadata={'url': 'https://gpt-index.readthedocs.io/en/latest'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='767b9945a76c081d709c631a4c4bd5ea20b3b2ca30c6b52133fa78638aab3be3', text='LlamaIndex is a data framework for LLM-based applications to ingest, structure, and access private or domain-specific data. It’s available in Python (these docs) and Typescript.\\n🚀 Why LlamaIndex?\\uf0c1\\nLLMs offer a natural language interface between humans and data. Widely available models come pre-trained on huge amounts of publicly available data like Wikipedia, mailing lists, textbooks, source code and more.\\nHowever, while LLMs are trained on a great deal of data, they are not trained on your data, which may be private or specific to the problem you’re trying to solve. It’s behind APIs, in SQL databases, or trapped in PDFs and slide decks.\\nYou may choose to fine-tune a LLM with your data, but:\\nTraining a LLM is expensive.\\nDue to the cost to train, it’s hard to update a LLM with latest information.\\nObservability is lacking. When you ask a LLM a question, it’s not obvious how the LLM arrived at its answer.\\nLlamaIndex takes a different approach called Retrieval-Augmented Generation (RAG). Instead of asking LLM to generate an answer immediately, LlamaIndex:\\nretrieves information from your data sources first,\\nadds it to your question as context, and\\nasks the LLM to answer based on the enriched prompt.\\nRAG overcomes all three weaknesses of the fine-tuning approach:\\nThere’s no training involved, so it’s cheap.\\nData is fetched only when you ask for them, so it’s always up to date.\\nLlamaIndex can show you the retrieved documents, so it’s more trustworthy.\\nLlamaIndex imposes no restriction on how you use LLMs. You can still use LLMs as auto-complete, chatbots, semi-autonomous agents, and more (see Use Cases on the left). It only makes LLMs more relevant to you.\\n🦙 How can LlamaIndex help?\\uf0c1\\nLlamaIndex provides the following tools:\\nData connectors ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more.\\nData indexes structure your data in intermediate representations that are easy and performant for LLMs to consume.\\nEngines provide natural language access to your data. For example:\\nQuery engines are powerful retrieval interfaces for knowledge-augmented output.\\nChat engines are conversational interfaces for multi-message, “back and forth” interactions with your data.\\nData agents are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more.\\nApplication integrations tie LlamaIndex back into the rest of your ecosystem. This could be LangChain, Flask, Docker, ChatGPT, or… anything else!\\n👨\\u200d👩\\u200d👧\\u200d👦 Who is LlamaIndex for?\\uf0c1\\nLlamaIndex provides tools for beginners, advanced users, and everyone in between.\\nOur high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code.\\nFor more complex applications, our lower-level APIs allow advanced users to customize and extend any module—data connectors, indices, retrievers, query engines, reranking modules—to fit their needs.\\nGetting Started\\uf0c1\\nTo install the library:\\npip install llama-index\\nWe recommend starting at how to read these docs, which will point you to the right place based on your experience level.\\n🗺️ Ecosystem\\uf0c1\\nTo download or contribute, find LlamaIndex on:\\nGithub: https://github.com/jerryjliu/llama_index\\nPyPi:\\nLlamaIndex: https://pypi.org/project/llama-index/.\\nGPT Index (duplicate): https://pypi.org/project/gpt-index/.\\nNPM (Typescript/Javascript):\\nGithub: https://github.com/run-llama/LlamaIndexTS\\nDocs: https://ts.llamaindex.ai/\\nLlamaIndex.TS: https://www.npmjs.com/package/llamaindex\\nAssociated projects\\uf0c1\\n🏡 LlamaHub: https://llamahub.ai | A large (and growing!) collection of custom data connectors\\n🧪 LlamaLab: https://github.com/run-llama/llama-lab | Ambitious projects built on top of LlamaIndex', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dc4f05e-50f0-4407-b04c-8fe3fd7e6725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6564db30-f56d-49ff-a359-1125eff5a182",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='23866397-0d5f-4e02-b705-a5e6d425319c', embedding=None, metadata={'url': 'https://docs.llamaindex.ai/en/latest/examples/query_engine/sub_question_query_engine.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='08eb1f1778c927a93d31cf7c99d603416ed32583fdce9c16000074dbe00b0a3f', text='Back to top \\nToggle table of contents sidebar\\nSub Question Query Engine\\uf0c1\\nIn this tutorial, we showcase how to use a sub question query engine to tackle the problem of answering a complex query using multiple data sources.\\nIt first breaks down the complex query into sub questions for each relevant data source, then gather all the intermediate reponses and synthesizes a final response.\\nPreparation\\uf0c1\\nIf you’re opening this Notebook on colab, you will probably need to install LlamaIndex 🦙.\\nimport os\\nimport openai\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\\n\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\\nfrom llama_index.tools import QueryEngineTool, ToolMetadata\\nfrom llama_index.query_engine import SubQuestionQueryEngine\\nfrom llama_index.callbacks import CallbackManager, LlamaDebugHandler\\nfrom llama_index import ServiceContext\\n# Using the LlamaDebugHandler to print the trace of the sub questions\\n# captured by the SUB_QUESTION callback event type\\nllama_debug = LlamaDebugHandler(print_trace_on_end=True)\\ncallback_manager = CallbackManager([llama_debug])\\nservice_context = ServiceContext.from_defaults(\\n    callback_manager=callback_manager\\n)\\nDownload Data\\uf0c1\\n!mkdir -p \\'data/paul_graham/\\'\\n!wget \\'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\\' -O \\'data/paul_graham/paul_graham_essay.txt\\'\\n# load data\\npg_essay = SimpleDirectoryReader(input_dir=\"./data/paul_graham/\").load_data()\\n\\n# build index and query engine\\nvector_query_engine = VectorStoreIndex.from_documents(\\n    pg_essay, use_async=True, service_context=service_context\\n).as_query_engine()\\n**********\\nTrace: index_construction\\n    |_node_parsing ->  0.394271 seconds\\n      |_chunking ->  0.393344 seconds\\n    |_embedding ->  0.753133 seconds\\n    |_embedding ->  0.749828 seconds\\n**********\\nSetup sub question query engine\\uf0c1\\n# setup base query engine as tool\\nquery_engine_tools = [\\n    QueryEngineTool(\\n        query_engine=vector_query_engine,\\n        metadata=ToolMetadata(\\n            name=\"pg_essay\",\\n            description=\"Paul Graham essay on What I Worked On\",\\n        ),\\n    ),\\n]\\n\\nquery_engine = SubQuestionQueryEngine.from_defaults(\\n    query_engine_tools=query_engine_tools,\\n    service_context=service_context,\\n    use_async=True,\\n)\\nRun queries\\uf0c1\\nresponse = query_engine.query(\\n    \"How was Paul Grahams life different before, during, and after YC?\"\\n)\\nGenerated 3 sub questions.\\n[pg_essay] Q: What did Paul Graham do before YC?\\n[pg_essay] Q: What did Paul Graham do during YC?\\n[pg_essay] Q: What did Paul Graham do after YC?\\n[pg_essay] A: \\nBefore YC, Paul Graham was a hacker, writer, and worked on Arc, a programming language. He also wrote essays and worked on other projects.\\n[pg_essay] A: \\nPaul Graham stopped working on YC in March 2014 and began painting. He spent most of the rest of the year painting and then in November he ran out of steam and stopped. He then began writing essays again and in March 2015 he started working on Lisp again.\\n[pg_essay] A: \\nPaul Graham worked on YC in a variety of ways. He wrote essays, worked on internal software in Arc, and created Hacker News. He also helped select and support founders, dealt with disputes between cofounders, and fought with people who maltreated the startups. He worked hard even at the parts he didn\\'t like, and was determined to make YC a success. In 2010, he was offered unsolicited advice to make sure YC wasn\\'t the last cool thing he did, which set him thinking about his future. In 2012, he decided to hand YC over to someone else and recruited Sam Altman to take over. He worked on YC until March 2014, when his mother passed away, and then he checked out completely.\\n**********\\nTrace: query\\n    |_query ->  13.064431 seconds\\n      |_llm ->  2.499768 seconds\\n      |_sub_question ->  2.05934 seconds\\n        |_query ->  2.059142 seconds\\n          |_retrieve ->  0.278184 seconds\\n            |_embedding ->  0.274593 seconds\\n          |_synthesize ->  1.780895 seconds\\n            |_llm ->  1.740488 seconds\\n      |_sub_question ->  5.364061 seconds\\n        |_query ->  5.363695 seconds\\n          |_retrieve ->  0.230257 seconds\\n            |_embedding ->  0.226763 seconds\\n          |_synthesize ->  5.133343 seconds\\n            |_llm ->  5.091069 seconds\\n      |_sub_question ->  2.148964 seconds\\n        |_query ->  2.14889 seconds\\n          |_retrieve ->  0.323438 seconds\\n            |_embedding ->  0.319841 seconds\\n          |_synthesize ->  1.825401 seconds\\n            |_llm ->  1.783064 seconds\\n      |_synthesize ->  5.198214 seconds\\n        |_llm ->  5.175849 seconds\\n**********\\nBefore YC, Paul Graham was a hacker, writer, and worked on Arc, a programming language. During YC, he wrote essays, worked on internal software in Arc, and created Hacker News. He also helped select and support founders, dealt with disputes between cofounders, and fought with people who maltreated the startups. After YC, Paul Graham stopped working on YC and began painting. He then began writing essays again and in March 2015 he started working on Lisp again. Paul Graham\\'s life was different before, during, and after YC in that he changed his focus from programming and writing to painting and then back to programming and writing.\\n# iterate through sub_question items captured in SUB_QUESTION event\\nfrom llama_index.callbacks.schema import CBEventType, EventPayload\\n\\nfor i, (start_event, end_event) in enumerate(\\n    llama_debug.get_event_pairs(CBEventType.SUB_QUESTION)\\n):\\n    qa_pair = end_event.payload[EventPayload.SUB_QUESTION]\\n    print(\"Sub Question \" + str(i) + \": \" + qa_pair.sub_q.sub_question.strip())\\n    print(\"Answer: \" + qa_pair.answer.strip())\\n    print(\"====================================\")\\nSub Question 0: What did Paul Graham do before YC?\\nAnswer: Before YC, Paul Graham was a hacker, writer, and worked on Arc, a programming language. He also wrote essays and worked on other projects.\\n====================================\\nSub Question 1: What did Paul Graham do during YC?\\nAnswer: Paul Graham worked on YC in a variety of ways. He wrote essays, worked on internal software in Arc, and created Hacker News. He also helped select and support founders, dealt with disputes between cofounders, and fought with people who maltreated the startups. He worked hard even at the parts he didn\\'t like, and was determined to make YC a success. In 2010, he was offered unsolicited advice to make sure YC wasn\\'t the last cool thing he did, which set him thinking about his future. In 2012, he decided to hand YC over to someone else and recruited Sam Altman to take over. He worked on YC until March 2014, when his mother passed away, and then he checked out completely.\\n====================================\\nSub Question 2: What did Paul Graham do after YC?\\nAnswer: Paul Graham stopped working on YC in March 2014 and began painting. He spent most of the rest of the year painting and then in November he ran out of steam and stopped. He then began writing essays again and in March 2015 he started working on Lisp again.\\n====================================', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ecb8b-2776-4a5e-9e6a-ed9264fe3157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
