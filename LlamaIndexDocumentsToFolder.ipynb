{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82ca9ade-ba07-47bf-88a5-dc9a65f3e86d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.readers.schema import Document\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d5eb1ff-ed0b-4b5b-b4e8-a96cd03156bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"documents.pkl\"\n",
    "filepath = \"\"\n",
    "fileuri = filepath+filename\n",
    "docs_folder_name = \"Documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dafe8ec-c765-4152-b2e6-d6415e9c0881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(fileuri, 'rb') as documents: \n",
    "    docs = pickle.load(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8446d49-d6cd-44a3-a3b5-ab69b39fbbfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='398cc74a-580d-41c2-a85f-0b980e4ff0ef', embedding=None, metadata={'url': 'https://gpt-index.readthedocs.io/en/latest'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='767b9945a76c081d709c631a4c4bd5ea20b3b2ca30c6b52133fa78638aab3be3', text='LlamaIndex is a data framework for LLM-based applications to ingest, structure, and access private or domain-specific data. It‚Äôs available in Python (these docs) and Typescript.\\nüöÄ Why LlamaIndex?\\uf0c1\\nLLMs offer a natural language interface between humans and data. Widely available models come pre-trained on huge amounts of publicly available data like Wikipedia, mailing lists, textbooks, source code and more.\\nHowever, while LLMs are trained on a great deal of data, they are not trained on your data, which may be private or specific to the problem you‚Äôre trying to solve. It‚Äôs behind APIs, in SQL databases, or trapped in PDFs and slide decks.\\nYou may choose to fine-tune a LLM with your data, but:\\nTraining a LLM is expensive.\\nDue to the cost to train, it‚Äôs hard to update a LLM with latest information.\\nObservability is lacking. When you ask a LLM a question, it‚Äôs not obvious how the LLM arrived at its answer.\\nLlamaIndex takes a different approach called Retrieval-Augmented Generation (RAG). Instead of asking LLM to generate an answer immediately, LlamaIndex:\\nretrieves information from your data sources first,\\nadds it to your question as context, and\\nasks the LLM to answer based on the enriched prompt.\\nRAG overcomes all three weaknesses of the fine-tuning approach:\\nThere‚Äôs no training involved, so it‚Äôs cheap.\\nData is fetched only when you ask for them, so it‚Äôs always up to date.\\nLlamaIndex can show you the retrieved documents, so it‚Äôs more trustworthy.\\nLlamaIndex imposes no restriction on how you use LLMs. You can still use LLMs as auto-complete, chatbots, semi-autonomous agents, and more (see Use Cases on the left). It only makes LLMs more relevant to you.\\nü¶ô How can LlamaIndex help?\\uf0c1\\nLlamaIndex provides the following tools:\\nData connectors ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more.\\nData indexes structure your data in intermediate representations that are easy and performant for LLMs to consume.\\nEngines provide natural language access to your data. For example:\\nQuery engines are powerful retrieval interfaces for knowledge-augmented output.\\nChat engines are conversational interfaces for multi-message, ‚Äúback and forth‚Äù interactions with your data.\\nData agents are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more.\\nApplication integrations tie LlamaIndex back into the rest of your ecosystem. This could be LangChain, Flask, Docker, ChatGPT, or‚Ä¶ anything else!\\nüë®\\u200düë©\\u200düëß\\u200düë¶ Who is LlamaIndex for?\\uf0c1\\nLlamaIndex provides tools for beginners, advanced users, and everyone in between.\\nOur high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code.\\nFor more complex applications, our lower-level APIs allow advanced users to customize and extend any module‚Äîdata connectors, indices, retrievers, query engines, reranking modules‚Äîto fit their needs.\\nGetting Started\\uf0c1\\nTo install the library:\\npip install llama-index\\nWe recommend starting at how to read these docs, which will point you to the right place based on your experience level.\\nüó∫Ô∏è Ecosystem\\uf0c1\\nTo download or contribute, find LlamaIndex on:\\nGithub: https://github.com/jerryjliu/llama_index\\nPyPi:\\nLlamaIndex: https://pypi.org/project/llama-index/.\\nGPT Index (duplicate): https://pypi.org/project/gpt-index/.\\nNPM (Typescript/Javascript):\\nGithub: https://github.com/run-llama/LlamaIndexTS\\nDocs: https://ts.llamaindex.ai/\\nLlamaIndex.TS: https://www.npmjs.com/package/llamaindex\\nAssociated projects\\uf0c1\\nüè° LlamaHub: https://llamahub.ai | A large (and growing!) collection of custom data connectors\\nüß™ LlamaLab: https://github.com/run-llama/llama-lab | Ambitious projects built on top of LlamaIndex', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dc22d5f-8acc-45aa-9a7e-10ef0a7c1ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://gpt-index.readthedocs.io/en/latest'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13577785-d6ae-41e6-a8dd-6090a161d5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents/398cc74a-580d-41c2-a85f-0b980e4ff0ef.txt\n",
      "Documents/22cf614d-8326-4c77-8976-34abcc621c20.txt\n",
      "Documents/5a2caaf7-40f4-4296-91d9-091648fe2656.txt\n",
      "Documents/84c5cebf-638e-4f48-9f2d-16b6fe166ed0.txt\n",
      "Documents/e6b77567-7108-41b4-834b-07b83c6fd15a.txt\n",
      "Documents/94491797-20af-4b3b-a3c8-09ad61b2c167.txt\n",
      "Documents/f3873480-6468-4bf9-9878-77b5a7151c87.txt\n",
      "Documents/23866397-0d5f-4e02-b705-a5e6d425319c.txt\n"
     ]
    }
   ],
   "source": [
    "for doc in docs: \n",
    "    docfileuri= docs_folder_name + '/' + doc.id_ +'.txt'\n",
    "    print(docfileuri)\n",
    "    with open(docfileuri, 'w') as textdoc: \n",
    "        textdoc.write(doc.text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4017d-9712-4389-89e1-e8f9954a07f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
