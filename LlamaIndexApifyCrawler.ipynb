{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a92097e-ff9e-4710-a8c8-99b3eb78fdf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "from llama_index.readers.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ed2828-c0c1-46f5-b0c7-6c477fe80e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tranform_dataset_item(item):\n",
    "    return Document(\n",
    "        text=item.get(\"text\"),\n",
    "        extra_info={\n",
    "            \"url\": item.get(\"url\"),\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f916b107-4d1f-4aa1-aa61-94b3272b841b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ApifyActor = download_loader(\"ApifyActor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98567838-69d1-4833-b563-1d328504f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "apify_api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa71978-0a00-4536-ac0d-6b68aae02f49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = ApifyActor(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f48ec97-751a-4bf0-8895-2e2519bd6c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = reader.load_data(\n",
    "    actor_id=\"apify/website-content-crawler\",\n",
    "    run_input={\"startUrls\": [{\"url\": \"https://gpt-index.readthedocs.io/en/latest\"}]},\n",
    "    dataset_mapping_function=tranform_dataset_item,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec117268-d4f4-471d-b391-93ad3490a019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38b5d35e-d6b9-4293-a443-38030965d1fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('documents.pkl', 'wb') as outp:\n",
    "    pickle.dump(documents, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1abfe10-dc0f-40a9-9011-c0c399b4a5b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('documents.pkl', 'rb') as pickle_file:\n",
    "    docs = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0201e621-3567-4367-bc93-d830ebf896ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='398cc74a-580d-41c2-a85f-0b980e4ff0ef', embedding=None, metadata={'url': 'https://gpt-index.readthedocs.io/en/latest'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='767b9945a76c081d709c631a4c4bd5ea20b3b2ca30c6b52133fa78638aab3be3', text='LlamaIndex is a data framework for LLM-based applications to ingest, structure, and access private or domain-specific data. It‚Äôs available in Python (these docs) and Typescript.\\nüöÄ Why LlamaIndex?\\uf0c1\\nLLMs offer a natural language interface between humans and data. Widely available models come pre-trained on huge amounts of publicly available data like Wikipedia, mailing lists, textbooks, source code and more.\\nHowever, while LLMs are trained on a great deal of data, they are not trained on your data, which may be private or specific to the problem you‚Äôre trying to solve. It‚Äôs behind APIs, in SQL databases, or trapped in PDFs and slide decks.\\nYou may choose to fine-tune a LLM with your data, but:\\nTraining a LLM is expensive.\\nDue to the cost to train, it‚Äôs hard to update a LLM with latest information.\\nObservability is lacking. When you ask a LLM a question, it‚Äôs not obvious how the LLM arrived at its answer.\\nLlamaIndex takes a different approach called Retrieval-Augmented Generation (RAG). Instead of asking LLM to generate an answer immediately, LlamaIndex:\\nretrieves information from your data sources first,\\nadds it to your question as context, and\\nasks the LLM to answer based on the enriched prompt.\\nRAG overcomes all three weaknesses of the fine-tuning approach:\\nThere‚Äôs no training involved, so it‚Äôs cheap.\\nData is fetched only when you ask for them, so it‚Äôs always up to date.\\nLlamaIndex can show you the retrieved documents, so it‚Äôs more trustworthy.\\nLlamaIndex imposes no restriction on how you use LLMs. You can still use LLMs as auto-complete, chatbots, semi-autonomous agents, and more (see Use Cases on the left). It only makes LLMs more relevant to you.\\nü¶ô How can LlamaIndex help?\\uf0c1\\nLlamaIndex provides the following tools:\\nData connectors ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more.\\nData indexes structure your data in intermediate representations that are easy and performant for LLMs to consume.\\nEngines provide natural language access to your data. For example:\\nQuery engines are powerful retrieval interfaces for knowledge-augmented output.\\nChat engines are conversational interfaces for multi-message, ‚Äúback and forth‚Äù interactions with your data.\\nData agents are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more.\\nApplication integrations tie LlamaIndex back into the rest of your ecosystem. This could be LangChain, Flask, Docker, ChatGPT, or‚Ä¶ anything else!\\nüë®\\u200düë©\\u200düëß\\u200düë¶ Who is LlamaIndex for?\\uf0c1\\nLlamaIndex provides tools for beginners, advanced users, and everyone in between.\\nOur high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code.\\nFor more complex applications, our lower-level APIs allow advanced users to customize and extend any module‚Äîdata connectors, indices, retrievers, query engines, reranking modules‚Äîto fit their needs.\\nGetting Started\\uf0c1\\nTo install the library:\\npip install llama-index\\nWe recommend starting at how to read these docs, which will point you to the right place based on your experience level.\\nüó∫Ô∏è Ecosystem\\uf0c1\\nTo download or contribute, find LlamaIndex on:\\nGithub: https://github.com/jerryjliu/llama_index\\nPyPi:\\nLlamaIndex: https://pypi.org/project/llama-index/.\\nGPT Index (duplicate): https://pypi.org/project/gpt-index/.\\nNPM (Typescript/Javascript):\\nGithub: https://github.com/run-llama/LlamaIndexTS\\nDocs: https://ts.llamaindex.ai/\\nLlamaIndex.TS: https://www.npmjs.com/package/llamaindex\\nAssociated projects\\uf0c1\\nüè° LlamaHub: https://llamahub.ai | A large (and growing!) collection of custom data connectors\\nüß™ LlamaLab: https://github.com/run-llama/llama-lab | Ambitious projects built on top of LlamaIndex', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='22cf614d-8326-4c77-8976-34abcc621c20', embedding=None, metadata={'url': 'https://docs.llamaindex.ai/en/latest/examples/retrievers/router_retriever.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='3134f55a9196025ecabae283c86228358f45cf7a495a25dcb28c5756bddb98d4', text=\"Selecting retriever 0: This choice is most relevant as it mentions retrieving all context from the essay, which could include information about the author's life..\\nNode ID: 7d07d325-489e-4157-a745-270e2066a643\\nSimilarity: None\\nText: What I Worked On\\nFebruary 2021\\nBefore college the two main things I worked on, outside of schoo‚Ä¶\\nNode ID: 01f0900b-db83-450b-a088-0473f16882d7\\nSimilarity: None\\nText: showed Terry Winograd using SHRDLU. I haven‚Äôt tried rereading The Moon is a Harsh Mistress, so I ‚Ä¶\\nNode ID: b2549a68-5fef-4179-b027-620ebfa6e346\\nSimilarity: None\\nText: Science is an uneasy alliance between two halves, theory and systems. The theory people prove thi‚Ä¶\\nNode ID: 4f1e9f0d-9bc6-4169-b3b6-4f169bbfa391\\nSimilarity: None\\nText: been explored. But all I wanted was to get out of grad school, and my rapidly written dissertatio‚Ä¶\\nNode ID: e20c99f9-5e80-4c92-8cc0-03d2a527131e\\nSimilarity: None\\nText: stop there, of course, or you get merely photographic accuracy, and what makes a still life inter‚Ä¶\\nNode ID: dbdf341a-f340-49f9-961f-16b9a51eea2d\\nSimilarity: None\\nText: that big, bureaucratic customers are a dangerous source of money, and that there‚Äôs not much overl‚Ä¶\\nNode ID: ed341d3a-9dda-49c1-8611-0ab40d04f08a\\nSimilarity: None\\nText: about money, because I could sense that Interleaf was on the way down. Freelance Lisp hacking wor‚Ä¶\\nNode ID: d69e02d3-2732-4567-a360-893c14ae157b\\nSimilarity: None\\nText: a web app, is common now, but at the time it wasn‚Äôt clear that it was even possible. To find out,‚Ä¶\\nNode ID: df9e00a5-e795-40a1-9a6b-8184d1b1e7c0\\nSimilarity: None\\nText: have to integrate with any other software except Robert‚Äôs and Trevor‚Äôs, so it was quite fun to wo‚Ä¶\\nNode ID: 38f2699b-0878-499b-90ee-821cb77e387b\\nSimilarity: None\\nText: all too keenly aware of the near-death experiences we seemed to have every few months. Nor had I ‚Ä¶\\nNode ID: be04d6a9-1fc7-4209-9df2-9c17a453699a\\nSimilarity: None\\nText: for a second still life, painted from the same objects (which hopefully hadn‚Äôt rotted yet).\\nMean‚Ä¶\\nNode ID: 42344911-8a7c-4e9b-81a8-0fcf40ab7690\\nSimilarity: None\\nText: which I‚Äôd created years before using Viaweb but had never used for anything. In one day it got 30‚Ä¶\\nNode ID: 9ec3df49-abf9-47f4-b0c2-16687882742a\\nSimilarity: None\\nText: I didn‚Äôt know but would turn out to like a lot: a woman called Jessica Livingston. A couple days ‚Ä¶\\nNode ID: d0cf6975-5261-4fb2-aae3-f3230090fb64\\nSimilarity: None\\nText: of readers, but professional investors are thinking ‚ÄúWow, that means they got all the returns.‚Äù B‚Ä¶\\nNode ID: 607d0480-7eee-4fb4-965d-3cb585fda62c\\nSimilarity: None\\nText: to the ‚ÄúYC GDP,‚Äù but as YC grows this becomes less and less of a joke. Now lots of startups get t‚Ä¶\\nNode ID: 730a49c9-55f7-4416-ab91-1d0c96e704c8\\nSimilarity: None\\nText: So this set me thinking. It was true that on my current trajectory, YC would be the last thing I ‚Ä¶\\nNode ID: edbe8c67-e373-42bf-af98-276b559cc08b\\nSimilarity: None\\nText: operators you need? The Lisp that John McCarthy invented, or more accurately discovered, is an an‚Ä¶\\nNode ID: 175a4375-35ec-45a0-a90c-15611505096b\\nSimilarity: None\\nText: Like McCarthy‚Äôs original Lisp, it‚Äôs a spec rather than an implementation, although like McCarthy‚Äô‚Ä¶\\nNode ID: 0cb367f9-0aac-422b-9243-0eaa7be15090\\nSimilarity: None\\nText: must tell readers things they don‚Äôt already know, and some people dislike being told such things‚Ä¶.\\nNode ID: 67afd4f1-9fa1-4e76-87ac-23b115823e6c\\nSimilarity: None\\nText: 1960 paper.\\nBut if so there‚Äôs no reason to suppose that this is the limit of the language that m‚Ä¶\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5a2caaf7-40f4-4296-91d9-091648fe2656', embedding=None, metadata={'url': 'https://docs.llamaindex.ai/en/latest/optimizing/advanced_retrieval/query_transformations.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='f9ad3735d986869b0cfafbc214ad7dd8cb86805723826ad9b2a818567c2a72ed', text='Back to top \\nToggle table of contents sidebar\\nQuery Transformations\\uf0c1\\nLlamaIndex allows you to perform query transformations over your index structures. Query transformations are modules that will convert a query into another query. They can be single-step, as in the transformation is run once before the query is executed against an index.\\nThey can also be multi-step, as in:\\nThe query is transformed, executed against an index,\\nThe response is retrieved.\\nSubsequent queries are transformed/executed in a sequential fashion.\\nWe list some of our query transformations in more detail below.\\nUse Cases\\uf0c1\\nQuery transformations have multiple use cases:\\nTransforming an initial query into a form that can be more easily embedded (e.g. HyDE)\\nTransforming an initial query into a subquestion that can be more easily answered from the data (single-step query decomposition)\\nBreaking an initial query into multiple subquestions that can be more easily answered on their own. (multi-step query decomposition)\\nHyDE (Hypothetical Document Embeddings)\\uf0c1\\nHyDE is a technique where given a natural language query, a hypothetical document/answer is generated first. This hypothetical document is then used for embedding lookup rather than the raw query.\\nTo use HyDE, an example code snippet is shown below.\\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\\nfrom llama_index.indices.query.query_transform.base import HyDEQueryTransform\\nfrom llama_index.query_engine.transform_query_engine import (\\n    TransformQueryEngine,\\n)\\n\\n# load documents, build index\\ndocuments = SimpleDirectoryReader(\"../paul_graham_essay/data\").load_data()\\nindex = VectorStoreIndex(documents)\\n\\n# run query with HyDE query transform\\nquery_str = \"what did paul graham do after going to RISD\"\\nhyde = HyDEQueryTransform(include_original=True)\\nquery_engine = index.as_query_engine()\\nquery_engine = TransformQueryEngine(query_engine, query_transform=hyde)\\nresponse = query_engine.query(query_str)\\nprint(response)\\nCheck out our example notebook for a full walkthrough.\\nSingle-Step Query Decomposition\\uf0c1\\nSome recent approaches (e.g. self-ask, ReAct) have suggested that LLM‚Äôs perform better at answering complex questions when they break the question into smaller steps. We have found that this is true for queries that require knowledge augmentation as well.\\nIf your query is complex, different parts of your knowledge base may answer different ‚Äúsubqueries‚Äù around the overall query.\\nOur single-step query decomposition feature transforms a complicated question into a simpler one over the data collection to help provide a sub-answer to the original question.\\nThis is especially helpful over a composed graph. Within a composed graph, a query can be routed to multiple subindexes, each representing a subset of the overall knowledge corpus. Query decomposition allows us to transform the query into a more suitable question over any given index.\\nAn example image is shown below.\\nHere‚Äôs a corresponding example code snippet over a composed graph.\\n# Setting: a summary index composed over multiple vector indices\\n# llm_chatgpt corresponds to the ChatGPT LLM interface\\nfrom llama_index.indices.query.query_transform.base import (\\n    DecomposeQueryTransform,\\n)\\n\\ndecompose_transform = DecomposeQueryTransform(llm_chatgpt, verbose=True)\\n\\n# initialize indexes and graph\\n...\\n\\n\\n# configure retrievers\\nvector_query_engine = vector_index.as_query_engine()\\nvector_query_engine = TransformQueryEngine(\\n    vector_query_engine,\\n    query_transform=decompose_transform,\\n    transform_extra_info={\"index_summary\": vector_index.index_struct.summary},\\n)\\ncustom_query_engines = {vector_index.index_id: vector_query_engine}\\n\\n# query\\nquery_str = (\\n    \"Compare and contrast the airports in Seattle, Houston, and Toronto. \"\\n)\\nquery_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\\nresponse = query_engine.query(query_str)\\nCheck out our example notebook for a full walkthrough.\\nMulti-Step Query Transformations\\uf0c1\\nMulti-step query transformations are a generalization on top of existing single-step query transformation approaches.\\nGiven an initial, complex query, the query is transformed and executed against an index. The response is retrieved from the query. Given the response (along with prior responses) and the query, follow-up questions may be asked against the index as well. This technique allows a query to be run against a single knowledge source until that query has satisfied all questions.\\nAn example image is shown below.\\nHere‚Äôs a corresponding example code snippet.\\nfrom llama_index.indices.query.query_transform.base import (\\n    StepDecomposeQueryTransform,\\n)\\n\\n# gpt-4\\nstep_decompose_transform = StepDecomposeQueryTransform(llm, verbose=True)\\n\\nquery_engine = index.as_query_engine()\\nquery_engine = MultiStepQueryEngine(\\n    query_engine, query_transform=step_decompose_transform\\n)\\n\\nresponse = query_engine.query(\\n    \"Who was in the first batch of the accelerator program the author started?\",\\n)\\nprint(str(response))\\nCheck out our example notebook for a full walkthrough.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='84c5cebf-638e-4f48-9f2d-16b6fe166ed0', embedding=None, metadata={'url': 'https://docs.llamaindex.ai/en/latest/examples/query_transformations/HyDEQueryTransformDemo.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='d03a96d412e7bd86439dc025c7af483e7e2547f84d15bb7e01fc029a09be281b', text='Back to top \\nToggle table of contents sidebar\\nHyDE Query Transform\\uf0c1\\nIf you‚Äôre opening this Notebook on colab, you will probably need to install LlamaIndex ü¶ô.\\nDownload Data\\uf0c1\\n!mkdir -p \\'data/paul_graham/\\'\\n!wget \\'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\\' -O \\'data/paul_graham/paul_graham_essay.txt\\'\\nLoad documents, build the VectorStoreIndex\\uf0c1\\nimport logging\\nimport sys\\n\\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\\n\\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\\nfrom llama_index.indices.query.query_transform import HyDEQueryTransform\\nfrom llama_index.query_engine.transform_query_engine import (\\n    TransformQueryEngine,\\n)\\nfrom IPython.display import Markdown, display\\n# load documents\\ndocuments = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\\nindex = VectorStoreIndex.from_documents(documents)\\nExample: HyDE improves specific temporal queries\\uf0c1\\nquery_str = \"what did paul graham do after going to RISD\"\\nFirst, we query without transformation: The same query string is used for embedding lookup and also summarization.\\uf0c1\\nquery_engine = index.as_query_engine()\\nresponse = query_engine.query(query_str)\\ndisplay(Markdown(f\"<b>{response}</b>\"))\\nAfter going to RISD, Paul Graham continued to pursue his passion for painting and art. He took classes in the painting department at the Accademia di Belli Arti in Florence, and he also took the entrance exam for the school. He also continued to work on his book On Lisp, and he took on consulting work to make money. At the school, Paul Graham and the other students had an arrangement where the faculty wouldn‚Äôt require the students to learn anything, and in return the students wouldn‚Äôt require the faculty to teach anything. Paul Graham was one of the few students who actually painted the nude model that was provided, while the rest of the students spent their time chatting or occasionally trying to imitate things they‚Äôd seen in American art magazines. The model turned out to live just down the street from Paul Graham, and she made a living from a combination of modelling and making fakes for a local antique dealer.\\nNow, we use HyDEQueryTransform to generate a hypothetical document and use it for embedding lookup.\\uf0c1\\nhyde = HyDEQueryTransform(include_original=True)\\nhyde_query_engine = TransformQueryEngine(query_engine, hyde)\\nresponse = hyde_query_engine.query(query_str)\\ndisplay(Markdown(f\"<b>{response}</b>\"))\\nAfter going to RISD, Paul Graham worked as a consultant for Interleaf and then co-founded Viaweb with Robert Morris. They created a software that allowed users to build websites via the web and received $10,000 in seed funding from Idelle‚Äôs husband Julian. They gave Julian 10% of the company in return for the initial legal work and business advice. Paul Graham had a negative net worth due to taxes he owed, so the seed funding was necessary for him to live on. They opened for business in January 1996 with 6 stores.\\nPaul Graham then left Yahoo after his options vested and went back to New York. He resumed his old life, but now he was rich. He tried to paint, but he didn‚Äôt have much energy or ambition. He eventually moved back to Cambridge and started working on a web app for making web apps. He recruited Dan Giffin and two undergrads to help him, but he eventually realized he didn‚Äôt want to run a company and decided to build a subset of the project as an open source project. He and Dan worked on a new dialect of Lisp, which he called Arc, in a house he bought in Cambridge. The subset he built as an open source project was the new Lisp, whose\\nIn this example, HyDE improves output quality significantly, by hallucinating accurately what Paul Graham did after RISD (see below), and thus improving the embedding quality, and final output.\\uf0c1\\nquery_bundle = hyde(query_str)\\nhyde_doc = query_bundle.embedding_strs[0]\\nAfter graduating from the Rhode Island School of Design (RISD) in 1985, Paul Graham went on to pursue a career in computer programming. He worked as a software developer for several companies, including Viaweb, which he co-founded in 1995. Viaweb was eventually acquired by Yahoo in 1998, and Graham used the proceeds to become a venture capitalist. He founded Y Combinator in 2005, a startup accelerator that has helped launch over 2,000 companies, including Dropbox, Airbnb, and Reddit. Graham has also written several books on programming and startups, and he continues to be an active investor in the tech industry.\\nFailure case 1: HyDE may mislead when query can be mis-interpreted without context.\\uf0c1\\nquery_str = \"What is Bel?\"\\nQuerying without transformation yields reasonable answer\\uf0c1\\nresponse = query_engine.query(query_str)\\ndisplay(Markdown(f\"<b>{response}</b>\"))\\nBel is a programming language that was written in Arc by Paul Graham over the course of four years (March 26, 2015 to October 12, 2019). It is based on John McCarthy‚Äôs original Lisp, but with additional features added. It is a spec expressed as code, and is meant to be a formal model of computation, an alternative to the Turing machine.\\nQuerying with HyDEQueryTransform results in nonsense\\uf0c1\\nhyde = HyDEQueryTransform(include_original=True)\\nhyde_query_engine = TransformQueryEngine(query_engine, hyde)\\nresponse = hyde_query_engine.query(query_str)\\ndisplay(Markdown(f\"<b>{response}</b>\"))\\nBel is the pseudonym of Paul Graham, the author of the context information who was in need of seed funding to live on and was part of a deal that became the model for Y Combinator‚Äôs.\\nIn this example, HyDE mis-interprets Bel without document context (see below), resulting in a completely unrelated embedding string and poor retrieval outcome.\\uf0c1\\nquery_bundle = hyde(query_str)\\nhyde_doc = query_bundle.embedding_strs[0]\\nBel is an ancient Semitic god, originating from the Middle East. He is often associated with the sun and is sometimes referred to as the ‚ÄúLord of Heaven‚Äù. Bel is also known as the god of fertility, abundance, and prosperity. He is often depicted as a bull or a man with a bull‚Äôs head. In some cultures, Bel is seen as a creator god, responsible for the creation of the universe. He is also associated with the underworld and is sometimes seen as a god of death. Bel is also associated with justice and is often seen as a protector of the innocent. Bel is an important figure in many religions, including Judaism, Christianity, and Islam.\\nFailure case 2: HyDE may bias open-ended queries\\uf0c1\\nquery_str = \"What would the author say about art vs. engineering?\"\\nQuerying without transformation yields a reasonable answer\\uf0c1\\nresponse = query_engine.query(query_str)\\ndisplay(Markdown(f\"<b>{response}</b>\"))\\nThe author would likely say that art and engineering are two different disciplines that require different skills and approaches. Art is more focused on expression and creativity, while engineering is more focused on problem-solving and technical knowledge. The author also suggests that art school does not always provide the same level of rigor as engineering school, and that painting students are often encouraged to develop a signature style rather than learn the fundamentals of painting. Furthermore, the author would likely point out that engineering can provide more financial stability than art, as evidenced by the author‚Äôs own experience of needing seed funding to live on while launching a company.\\nQuerying with HyDEQueryTransform results in a more biased output\\uf0c1\\nresponse = hyde_query_engine.query(query_str)\\ndisplay(Markdown(f\"<b>{response}</b>\"))\\nThe author would likely say that art is a more lasting and independent form of work than engineering. They mention that software written today will be obsolete in a couple decades, and that systems work does not last. In contrast, they note that paintings can last hundreds of years and that it is possible to make a living as an artist. They also mention that as an artist, you can be truly independent and don‚Äôt need to have a boss or research funding. Furthermore, they note that art can be a source of income for people who may not have access to traditional forms of employment, such as the model in the example who was able to make a living from modelling and making fakes for a local antique dealer.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e6b77567-7108-41b4-834b-07b83c6fd15a', embedding=None, metadata={'url': 'https://docs.llamaindex.ai/en/latest/examples/query_transformations/SimpleIndexDemo-multistep.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='6589026e61851f546c88077c1d9da5a97c86027aae6dba2eb7fe44d200ec241c', text='Back to top \\nToggle table of contents sidebar\\nMulti-Step Query Engine\\uf0c1\\nWe have a multi-step query engine that‚Äôs able to decompose a complex query into sequential subquestions. This guide walks you through how to set it up!\\nIf you‚Äôre opening this Notebook on colab, you will probably need to install LlamaIndex ü¶ô.\\nDownload Data\\uf0c1\\n!mkdir -p \\'data/paul_graham/\\'\\n!wget \\'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\\' -O \\'data/paul_graham/paul_graham_essay.txt\\'\\nLoad documents, build the VectorStoreIndex\\uf0c1\\nimport logging\\nimport sys\\n\\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\\n\\nfrom llama_index import (\\n    VectorStoreIndex,\\n    SimpleDirectoryReader,\\n    ServiceContext,\\n)\\nfrom llama_index.llms import OpenAI\\nfrom IPython.display import Markdown, display\\n# LLM (gpt-3)\\ngpt3 = OpenAI(temperature=0, model=\"text-davinci-003\")\\nservice_context_gpt3 = ServiceContext.from_defaults(llm=gpt3)\\n\\n# LLM (gpt-4)\\ngpt4 = OpenAI(temperature=0, model=\"gpt-4\")\\nservice_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\\n# load documents\\ndocuments = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\\nindex = VectorStoreIndex.from_documents(documents)\\nQuery Index\\uf0c1\\nfrom llama_index.indices.query.query_transform.base import (\\n    StepDecomposeQueryTransform,\\n)\\n\\n# gpt-4\\nstep_decompose_transform = StepDecomposeQueryTransform(llm=gpt4, verbose=True)\\n\\n# gpt-3\\nstep_decompose_transform_gpt3 = StepDecomposeQueryTransform(\\n    llm=gpt3, verbose=True\\n)\\nindex_summary = \"Used to answer questions about the author\"\\n# set Logging to DEBUG for more detailed outputs\\nfrom llama_index.query_engine.multistep_query_engine import (\\n    MultiStepQueryEngine,\\n)\\n\\nquery_engine = index.as_query_engine(service_context=service_context_gpt4)\\nquery_engine = MultiStepQueryEngine(\\n    query_engine=query_engine,\\n    query_transform=step_decompose_transform,\\n    index_summary=index_summary,\\n)\\nresponse_gpt4 = query_engine.query(\\n    \"Who was in the first batch of the accelerator program the author\"\\n    \" started?\",\\n)\\ndisplay(Markdown(f\"<b>{response_gpt4}</b>\"))\\nsub_qa = response_gpt4.metadata[\"sub_qa\"]\\ntuples = [(t[0], t[1].response) for t in sub_qa]\\nprint(tuples)\\nresponse_gpt4 = query_engine.query(\\n    \"In which city did the author found his first company, Viaweb?\",\\n)\\nquery_engine = index.as_query_engine(service_context=service_context_gpt3)\\nquery_engine = MultiStepQueryEngine(\\n    query_engine=query_engine,\\n    query_transform=step_decompose_transform_gpt3,\\n    index_summary=index_summary,\\n)\\n\\nresponse_gpt3 = query_engine.query(\\n    \"In which city did the author found his first company, Viaweb?\",\\n)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='94491797-20af-4b3b-a3c8-09ad61b2c167', embedding=None, metadata={'url': 'https://docs.llamaindex.ai/en/latest/examples/ingestion/document_management_pipeline.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='28585da86c32b6e3e718c96e1c176133fc83daa2250845010840df70d7257236', text='Back to top \\nToggle table of contents sidebar\\nAttaching a docstore to the ingestion pipeline will enable document management.\\nUsing the document.doc_id or node.ref_doc_id as a grounding point, the ingestion pipeline will actively look for duplicate documents.\\nIt works by\\nStoring a map of doc_id -> document_hash\\nIf a duplicate doc_id is detected, and the hash has changed, the document will be re-processed\\nIf the hash has not changed, the document will be skipped in the pipeline\\nIf we do not attach a vector store, we can only check for and remove duplicate inputs.\\nIf a vector store is attached, we can also handle upserts! We have another guide for upserts and vector stores.\\nCreate Seed Data\\uf0c1\\n# Make some test data\\n!mkdir -p data\\n!echo \"This is a test file: one!\" > data/test1.txt\\n!echo \"This is a test file: two!\" > data/test2.txt\\nfrom llama_index import SimpleDirectoryReader\\n\\n# load documents with deterministic IDs\\ndocuments = SimpleDirectoryReader(\"./data\", filename_as_id=True).load_data()\\n/home/loganm/.cache/pypoetry/virtualenvs/llama-index-4a-wkI5X-py3.11/lib/python3.11/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.9) is available. It\\'s recommended that you update to the latest version using `pip install -U deeplake`.\\n  warnings.warn(\\nCreate Pipeline with Document Store\\uf0c1\\nfrom llama_index.embeddings import HuggingFaceEmbedding\\nfrom llama_index.ingestion import IngestionPipeline\\nfrom llama_index.storage.docstore import (\\n    SimpleDocumentStore,\\n    RedisDocumentStore,\\n    MongoDocumentStore,\\n)\\nfrom llama_index.text_splitter import SentenceSplitter\\n\\n\\npipeline = IngestionPipeline(\\n    transformations=[\\n        SentenceSplitter(),\\n        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\\n    ],\\n    docstore=SimpleDocumentStore(),\\n)\\nnodes = pipeline.run(documents=documents)\\nDocstore strategy set to upserts, but no vector store. Switching to duplicates_only strategy.\\nprint(f\"Ingested {len(nodes)} Nodes\")\\n[Optional] Save/Load Pipeline\\uf0c1\\nSaving the pipeline will save both the internal cache and docstore.\\nNOTE: If you were using remote caches/docstores, this step is not needed\\npipeline.persist(\"./pipeline_storage\")\\npipeline = IngestionPipeline(\\n    transformations=[\\n        SentenceSplitter(),\\n        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\\n    ]\\n)\\n\\n# restore the pipeline\\npipeline.load(\"./pipeline_storage\")\\nTest the Document Management\\uf0c1\\nHere, we can create a new document, as well as edit an existing document, to test the document management.\\nBoth the new document and edited document will be ingested, while the unchanged document will be skipped\\n!echo \"This is a test file: three!\" > data/test3.txt\\n!echo \"This is a NEW test file: one!\" > data/test1.txt\\ndocuments = SimpleDirectoryReader(\"./data\", filename_as_id=True).load_data()\\nnodes = pipeline.run(documents=documents)\\nDocstore strategy set to upserts, but no vector store. Switching to duplicates_only strategy.\\nprint(f\"Ingested {len(nodes)} Nodes\")\\nLets confirm which nodes were ingested:\\nfor node in nodes:\\n    print(f\"Node: {node.text}\")\\nNode: This is a NEW test file: one!\\nNode: This is a test file: three!\\nWe can also verify the docstore has only three documents tracked\\nprint(len(pipeline.docstore.docs))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f3873480-6468-4bf9-9878-77b5a7151c87', embedding=None, metadata={'url': 'https://docs.llamaindex.ai/en/latest/examples/ingestion/redis_ingestion_pipeline.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='1daaab7007394a426618070a7b2e3f223546c4c343da11abaf1897323500d790', text='Back to top \\nToggle table of contents sidebar\\nRedis Ingestion Pipeline\\uf0c1\\nThis walkthrough shows how to use Redis for both the vector store, cache, and docstore in an Ingestion Pipeline.\\nDependencies\\uf0c1\\nInstall and start redis, setup OpenAI API key\\nRequirement already satisfied: redis in /home/loganm/.cache/pypoetry/virtualenvs/llama-index-4a-wkI5X-py3.11/lib/python3.11/site-packages (5.0.1)\\nRequirement already satisfied: async-timeout>=4.0.2 in /home/loganm/.cache/pypoetry/virtualenvs/llama-index-4a-wkI5X-py3.11/lib/python3.11/site-packages (from redis) (4.0.3)\\n\\n[notice] A new release of pip is available: 23.2.1 -> 23.3.1\\n[notice] To update, run: pip install --upgrade pip\\n!docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\\n338c889086e8649aa80dfb79ebff4fffc98d72fc6d988ac158c6662e9e0cf04b\\nimport os\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\nCreate Seed Data\\uf0c1\\n# Make some test data\\n!rm -rf test_redis_data\\n!mkdir -p test_redis_data\\n!echo \"This is a test file: one!\" > test_redis_data/test1.txt\\n!echo \"This is a test file: two!\" > test_redis_data/test2.txt\\nfrom llama_index import SimpleDirectoryReader\\n\\n# load documents with deterministic IDs\\ndocuments = SimpleDirectoryReader(\\n    \"./test_redis_data\", filename_as_id=True\\n).load_data()\\n/home/loganm/.cache/pypoetry/virtualenvs/llama-index-4a-wkI5X-py3.11/lib/python3.11/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.9) is available. It\\'s recommended that you update to the latest version using `pip install -U deeplake`.\\n  warnings.warn(\\nRun the Redis-Based Ingestion Pipeline\\uf0c1\\nWith a vector store attached, the pipeline will handle upserting data into your vector store.\\nHowever, if you only want to handle duplcates, you can change the strategy to DUPLICATES_ONLY.\\nfrom llama_index.embeddings import HuggingFaceEmbedding\\nfrom llama_index.ingestion import (\\n    DocstoreStrategy,\\n    IngestionPipeline,\\n    IngestionCache,\\n)\\nfrom llama_index.ingestion.cache import RedisCache\\nfrom llama_index.storage.docstore import RedisDocumentStore\\nfrom llama_index.text_splitter import SentenceSplitter\\nfrom llama_index.vector_stores import RedisVectorStore\\n\\nembed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\\n\\npipeline = IngestionPipeline(\\n    transformations=[\\n        SentenceSplitter(),\\n        embed_model,\\n    ],\\n    docstore=RedisDocumentStore.from_host_and_port(\\n        \"localhost\", 6379, namespace=\"document_store\"\\n    ),\\n    vector_store=RedisVectorStore(\\n        index_name=\"redis_vector_store\",\\n        index_prefix=\"vectore_store\",\\n        redis_url=\"redis://localhost:6379\",\\n    ),\\n    cache=IngestionCache(\\n        cache=RedisCache.from_host_and_port(\"localhost\", 6379),\\n        collection=\"redis_cache\",\\n    ),\\n    docstore_strategy=DocstoreStrategy.UPSERTS,\\n)\\nnodes = pipeline.run(documents=documents)\\nprint(f\"Ingested {len(nodes)} Nodes\")\\nConfirm documents are ingested\\uf0c1\\nWe can create a vector index using our vector store, and quickly ask which documents are seen.\\nfrom llama_index import VectorStoreIndex, ServiceContext\\n\\nservice_context = ServiceContext.from_defaults(embed_model=embed_model)\\n\\nindex = VectorStoreIndex.from_vector_store(\\n    pipeline.vector_store, service_context=service_context\\n)\\nprint(\\n    index.as_query_engine(similarity_top_k=10).query(\\n        \"What documents do you see?\"\\n    )\\n)\\nI see two documents: \"test2.txt\" and \"test1.txt\".\\nAdd data and Ingest\\uf0c1\\nHere, we can update an existing file, as well as add a new one!\\n!echo \"This is a test file: three!\" > test_redis_data/test3.txt\\n!echo \"This is a NEW test file: one!\" > test_redis_data/test1.txt\\ndocuments = SimpleDirectoryReader(\\n    \"./test_redis_data\", filename_as_id=True\\n).load_data()\\n\\nnodes = pipeline.run(documents=documents)\\n\\nprint(f\"Ingested {len(nodes)} Nodes\")\\nindex = VectorStoreIndex.from_vector_store(\\n    pipeline.vector_store, service_context=service_context\\n)\\n\\nresponse = index.as_query_engine(similarity_top_k=10).query(\\n    \"What documents do you see?\"\\n)\\n\\nprint(response)\\n\\nfor node in response.source_nodes:\\n    print(node.get_text())\\nI see three documents: test3.txt, test1.txt, and test2.txt.\\nThis is a test file: three!\\nThis is a NEW test file: one!\\nThis is a test file: two!\\nAs we can see, the data was deduplicated and upserted correctly! Only three nodes are in the index, even though we ran the full pipeline twice.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='23866397-0d5f-4e02-b705-a5e6d425319c', embedding=None, metadata={'url': 'https://docs.llamaindex.ai/en/latest/examples/query_engine/sub_question_query_engine.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='08eb1f1778c927a93d31cf7c99d603416ed32583fdce9c16000074dbe00b0a3f', text='Back to top \\nToggle table of contents sidebar\\nSub Question Query Engine\\uf0c1\\nIn this tutorial, we showcase how to use a sub question query engine to tackle the problem of answering a complex query using multiple data sources.\\nIt first breaks down the complex query into sub questions for each relevant data source, then gather all the intermediate reponses and synthesizes a final response.\\nPreparation\\uf0c1\\nIf you‚Äôre opening this Notebook on colab, you will probably need to install LlamaIndex ü¶ô.\\nimport os\\nimport openai\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\\n\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\\nfrom llama_index.tools import QueryEngineTool, ToolMetadata\\nfrom llama_index.query_engine import SubQuestionQueryEngine\\nfrom llama_index.callbacks import CallbackManager, LlamaDebugHandler\\nfrom llama_index import ServiceContext\\n# Using the LlamaDebugHandler to print the trace of the sub questions\\n# captured by the SUB_QUESTION callback event type\\nllama_debug = LlamaDebugHandler(print_trace_on_end=True)\\ncallback_manager = CallbackManager([llama_debug])\\nservice_context = ServiceContext.from_defaults(\\n    callback_manager=callback_manager\\n)\\nDownload Data\\uf0c1\\n!mkdir -p \\'data/paul_graham/\\'\\n!wget \\'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\\' -O \\'data/paul_graham/paul_graham_essay.txt\\'\\n# load data\\npg_essay = SimpleDirectoryReader(input_dir=\"./data/paul_graham/\").load_data()\\n\\n# build index and query engine\\nvector_query_engine = VectorStoreIndex.from_documents(\\n    pg_essay, use_async=True, service_context=service_context\\n).as_query_engine()\\n**********\\nTrace: index_construction\\n    |_node_parsing ->  0.394271 seconds\\n      |_chunking ->  0.393344 seconds\\n    |_embedding ->  0.753133 seconds\\n    |_embedding ->  0.749828 seconds\\n**********\\nSetup sub question query engine\\uf0c1\\n# setup base query engine as tool\\nquery_engine_tools = [\\n    QueryEngineTool(\\n        query_engine=vector_query_engine,\\n        metadata=ToolMetadata(\\n            name=\"pg_essay\",\\n            description=\"Paul Graham essay on What I Worked On\",\\n        ),\\n    ),\\n]\\n\\nquery_engine = SubQuestionQueryEngine.from_defaults(\\n    query_engine_tools=query_engine_tools,\\n    service_context=service_context,\\n    use_async=True,\\n)\\nRun queries\\uf0c1\\nresponse = query_engine.query(\\n    \"How was Paul Grahams life different before, during, and after YC?\"\\n)\\nGenerated 3 sub questions.\\n[pg_essay] Q: What did Paul Graham do before YC?\\n[pg_essay] Q: What did Paul Graham do during YC?\\n[pg_essay] Q: What did Paul Graham do after YC?\\n[pg_essay] A: \\nBefore YC, Paul Graham was a hacker, writer, and worked on Arc, a programming language. He also wrote essays and worked on other projects.\\n[pg_essay] A: \\nPaul Graham stopped working on YC in March 2014 and began painting. He spent most of the rest of the year painting and then in November he ran out of steam and stopped. He then began writing essays again and in March 2015 he started working on Lisp again.\\n[pg_essay] A: \\nPaul Graham worked on YC in a variety of ways. He wrote essays, worked on internal software in Arc, and created Hacker News. He also helped select and support founders, dealt with disputes between cofounders, and fought with people who maltreated the startups. He worked hard even at the parts he didn\\'t like, and was determined to make YC a success. In 2010, he was offered unsolicited advice to make sure YC wasn\\'t the last cool thing he did, which set him thinking about his future. In 2012, he decided to hand YC over to someone else and recruited Sam Altman to take over. He worked on YC until March 2014, when his mother passed away, and then he checked out completely.\\n**********\\nTrace: query\\n    |_query ->  13.064431 seconds\\n      |_llm ->  2.499768 seconds\\n      |_sub_question ->  2.05934 seconds\\n        |_query ->  2.059142 seconds\\n          |_retrieve ->  0.278184 seconds\\n            |_embedding ->  0.274593 seconds\\n          |_synthesize ->  1.780895 seconds\\n            |_llm ->  1.740488 seconds\\n      |_sub_question ->  5.364061 seconds\\n        |_query ->  5.363695 seconds\\n          |_retrieve ->  0.230257 seconds\\n            |_embedding ->  0.226763 seconds\\n          |_synthesize ->  5.133343 seconds\\n            |_llm ->  5.091069 seconds\\n      |_sub_question ->  2.148964 seconds\\n        |_query ->  2.14889 seconds\\n          |_retrieve ->  0.323438 seconds\\n            |_embedding ->  0.319841 seconds\\n          |_synthesize ->  1.825401 seconds\\n            |_llm ->  1.783064 seconds\\n      |_synthesize ->  5.198214 seconds\\n        |_llm ->  5.175849 seconds\\n**********\\nBefore YC, Paul Graham was a hacker, writer, and worked on Arc, a programming language. During YC, he wrote essays, worked on internal software in Arc, and created Hacker News. He also helped select and support founders, dealt with disputes between cofounders, and fought with people who maltreated the startups. After YC, Paul Graham stopped working on YC and began painting. He then began writing essays again and in March 2015 he started working on Lisp again. Paul Graham\\'s life was different before, during, and after YC in that he changed his focus from programming and writing to painting and then back to programming and writing.\\n# iterate through sub_question items captured in SUB_QUESTION event\\nfrom llama_index.callbacks.schema import CBEventType, EventPayload\\n\\nfor i, (start_event, end_event) in enumerate(\\n    llama_debug.get_event_pairs(CBEventType.SUB_QUESTION)\\n):\\n    qa_pair = end_event.payload[EventPayload.SUB_QUESTION]\\n    print(\"Sub Question \" + str(i) + \": \" + qa_pair.sub_q.sub_question.strip())\\n    print(\"Answer: \" + qa_pair.answer.strip())\\n    print(\"====================================\")\\nSub Question 0: What did Paul Graham do before YC?\\nAnswer: Before YC, Paul Graham was a hacker, writer, and worked on Arc, a programming language. He also wrote essays and worked on other projects.\\n====================================\\nSub Question 1: What did Paul Graham do during YC?\\nAnswer: Paul Graham worked on YC in a variety of ways. He wrote essays, worked on internal software in Arc, and created Hacker News. He also helped select and support founders, dealt with disputes between cofounders, and fought with people who maltreated the startups. He worked hard even at the parts he didn\\'t like, and was determined to make YC a success. In 2010, he was offered unsolicited advice to make sure YC wasn\\'t the last cool thing he did, which set him thinking about his future. In 2012, he decided to hand YC over to someone else and recruited Sam Altman to take over. He worked on YC until March 2014, when his mother passed away, and then he checked out completely.\\n====================================\\nSub Question 2: What did Paul Graham do after YC?\\nAnswer: Paul Graham stopped working on YC in March 2014 and began painting. He spent most of the rest of the year painting and then in November he ran out of steam and stopped. He then began writing essays again and in March 2015 he started working on Lisp again.\\n====================================', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996520be-0791-4da4-b203-bf1f2292d004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
