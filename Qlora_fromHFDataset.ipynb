{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb57ffe-8604-48df-9479-52f2ff1518b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path =\"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45c28dce-bd89-458a-b701-13f4d9546684",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "        \"load_in_4bit\" : True           \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993a6045-9e57-43c9-bf17-a71e3cc0bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b55b8de-36b2-486f-b41c-c803e5469606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca76c37b-9640-4112-bd2f-6f37103645b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "embed_tokens\n",
      "layers\n",
      "layers.0\n",
      "layers.0.self_attn\n",
      "layers.0.self_attn.q_proj\n",
      "layers.0.self_attn.k_proj\n",
      "layers.0.self_attn.v_proj\n",
      "layers.0.self_attn.o_proj\n",
      "layers.0.self_attn.rotary_emb\n",
      "layers.0.mlp\n",
      "layers.0.mlp.gate_proj\n",
      "layers.0.mlp.up_proj\n",
      "layers.0.mlp.down_proj\n",
      "layers.0.mlp.act_fn\n",
      "layers.0.input_layernorm\n",
      "layers.0.post_attention_layernorm\n",
      "layers.1\n",
      "layers.1.self_attn\n",
      "layers.1.self_attn.q_proj\n",
      "layers.1.self_attn.k_proj\n",
      "layers.1.self_attn.v_proj\n",
      "layers.1.self_attn.o_proj\n",
      "layers.1.self_attn.rotary_emb\n",
      "layers.1.mlp\n",
      "layers.1.mlp.gate_proj\n",
      "layers.1.mlp.up_proj\n",
      "layers.1.mlp.down_proj\n",
      "layers.1.mlp.act_fn\n",
      "layers.1.input_layernorm\n",
      "layers.1.post_attention_layernorm\n",
      "layers.2\n",
      "layers.2.self_attn\n",
      "layers.2.self_attn.q_proj\n",
      "layers.2.self_attn.k_proj\n",
      "layers.2.self_attn.v_proj\n",
      "layers.2.self_attn.o_proj\n",
      "layers.2.self_attn.rotary_emb\n",
      "layers.2.mlp\n",
      "layers.2.mlp.gate_proj\n",
      "layers.2.mlp.up_proj\n",
      "layers.2.mlp.down_proj\n",
      "layers.2.mlp.act_fn\n",
      "layers.2.input_layernorm\n",
      "layers.2.post_attention_layernorm\n",
      "layers.3\n",
      "layers.3.self_attn\n",
      "layers.3.self_attn.q_proj\n",
      "layers.3.self_attn.k_proj\n",
      "layers.3.self_attn.v_proj\n",
      "layers.3.self_attn.o_proj\n",
      "layers.3.self_attn.rotary_emb\n",
      "layers.3.mlp\n",
      "layers.3.mlp.gate_proj\n",
      "layers.3.mlp.up_proj\n",
      "layers.3.mlp.down_proj\n",
      "layers.3.mlp.act_fn\n",
      "layers.3.input_layernorm\n",
      "layers.3.post_attention_layernorm\n",
      "layers.4\n",
      "layers.4.self_attn\n",
      "layers.4.self_attn.q_proj\n",
      "layers.4.self_attn.k_proj\n",
      "layers.4.self_attn.v_proj\n",
      "layers.4.self_attn.o_proj\n",
      "layers.4.self_attn.rotary_emb\n",
      "layers.4.mlp\n",
      "layers.4.mlp.gate_proj\n",
      "layers.4.mlp.up_proj\n",
      "layers.4.mlp.down_proj\n",
      "layers.4.mlp.act_fn\n",
      "layers.4.input_layernorm\n",
      "layers.4.post_attention_layernorm\n",
      "layers.5\n",
      "layers.5.self_attn\n",
      "layers.5.self_attn.q_proj\n",
      "layers.5.self_attn.k_proj\n",
      "layers.5.self_attn.v_proj\n",
      "layers.5.self_attn.o_proj\n",
      "layers.5.self_attn.rotary_emb\n",
      "layers.5.mlp\n",
      "layers.5.mlp.gate_proj\n",
      "layers.5.mlp.up_proj\n",
      "layers.5.mlp.down_proj\n",
      "layers.5.mlp.act_fn\n",
      "layers.5.input_layernorm\n",
      "layers.5.post_attention_layernorm\n",
      "layers.6\n",
      "layers.6.self_attn\n",
      "layers.6.self_attn.q_proj\n",
      "layers.6.self_attn.k_proj\n",
      "layers.6.self_attn.v_proj\n",
      "layers.6.self_attn.o_proj\n",
      "layers.6.self_attn.rotary_emb\n",
      "layers.6.mlp\n",
      "layers.6.mlp.gate_proj\n",
      "layers.6.mlp.up_proj\n",
      "layers.6.mlp.down_proj\n",
      "layers.6.mlp.act_fn\n",
      "layers.6.input_layernorm\n",
      "layers.6.post_attention_layernorm\n",
      "layers.7\n",
      "layers.7.self_attn\n",
      "layers.7.self_attn.q_proj\n",
      "layers.7.self_attn.k_proj\n",
      "layers.7.self_attn.v_proj\n",
      "layers.7.self_attn.o_proj\n",
      "layers.7.self_attn.rotary_emb\n",
      "layers.7.mlp\n",
      "layers.7.mlp.gate_proj\n",
      "layers.7.mlp.up_proj\n",
      "layers.7.mlp.down_proj\n",
      "layers.7.mlp.act_fn\n",
      "layers.7.input_layernorm\n",
      "layers.7.post_attention_layernorm\n",
      "layers.8\n",
      "layers.8.self_attn\n",
      "layers.8.self_attn.q_proj\n",
      "layers.8.self_attn.k_proj\n",
      "layers.8.self_attn.v_proj\n",
      "layers.8.self_attn.o_proj\n",
      "layers.8.self_attn.rotary_emb\n",
      "layers.8.mlp\n",
      "layers.8.mlp.gate_proj\n",
      "layers.8.mlp.up_proj\n",
      "layers.8.mlp.down_proj\n",
      "layers.8.mlp.act_fn\n",
      "layers.8.input_layernorm\n",
      "layers.8.post_attention_layernorm\n",
      "layers.9\n",
      "layers.9.self_attn\n",
      "layers.9.self_attn.q_proj\n",
      "layers.9.self_attn.k_proj\n",
      "layers.9.self_attn.v_proj\n",
      "layers.9.self_attn.o_proj\n",
      "layers.9.self_attn.rotary_emb\n",
      "layers.9.mlp\n",
      "layers.9.mlp.gate_proj\n",
      "layers.9.mlp.up_proj\n",
      "layers.9.mlp.down_proj\n",
      "layers.9.mlp.act_fn\n",
      "layers.9.input_layernorm\n",
      "layers.9.post_attention_layernorm\n",
      "layers.10\n",
      "layers.10.self_attn\n",
      "layers.10.self_attn.q_proj\n",
      "layers.10.self_attn.k_proj\n",
      "layers.10.self_attn.v_proj\n",
      "layers.10.self_attn.o_proj\n",
      "layers.10.self_attn.rotary_emb\n",
      "layers.10.mlp\n",
      "layers.10.mlp.gate_proj\n",
      "layers.10.mlp.up_proj\n",
      "layers.10.mlp.down_proj\n",
      "layers.10.mlp.act_fn\n",
      "layers.10.input_layernorm\n",
      "layers.10.post_attention_layernorm\n",
      "layers.11\n",
      "layers.11.self_attn\n",
      "layers.11.self_attn.q_proj\n",
      "layers.11.self_attn.k_proj\n",
      "layers.11.self_attn.v_proj\n",
      "layers.11.self_attn.o_proj\n",
      "layers.11.self_attn.rotary_emb\n",
      "layers.11.mlp\n",
      "layers.11.mlp.gate_proj\n",
      "layers.11.mlp.up_proj\n",
      "layers.11.mlp.down_proj\n",
      "layers.11.mlp.act_fn\n",
      "layers.11.input_layernorm\n",
      "layers.11.post_attention_layernorm\n",
      "layers.12\n",
      "layers.12.self_attn\n",
      "layers.12.self_attn.q_proj\n",
      "layers.12.self_attn.k_proj\n",
      "layers.12.self_attn.v_proj\n",
      "layers.12.self_attn.o_proj\n",
      "layers.12.self_attn.rotary_emb\n",
      "layers.12.mlp\n",
      "layers.12.mlp.gate_proj\n",
      "layers.12.mlp.up_proj\n",
      "layers.12.mlp.down_proj\n",
      "layers.12.mlp.act_fn\n",
      "layers.12.input_layernorm\n",
      "layers.12.post_attention_layernorm\n",
      "layers.13\n",
      "layers.13.self_attn\n",
      "layers.13.self_attn.q_proj\n",
      "layers.13.self_attn.k_proj\n",
      "layers.13.self_attn.v_proj\n",
      "layers.13.self_attn.o_proj\n",
      "layers.13.self_attn.rotary_emb\n",
      "layers.13.mlp\n",
      "layers.13.mlp.gate_proj\n",
      "layers.13.mlp.up_proj\n",
      "layers.13.mlp.down_proj\n",
      "layers.13.mlp.act_fn\n",
      "layers.13.input_layernorm\n",
      "layers.13.post_attention_layernorm\n",
      "layers.14\n",
      "layers.14.self_attn\n",
      "layers.14.self_attn.q_proj\n",
      "layers.14.self_attn.k_proj\n",
      "layers.14.self_attn.v_proj\n",
      "layers.14.self_attn.o_proj\n",
      "layers.14.self_attn.rotary_emb\n",
      "layers.14.mlp\n",
      "layers.14.mlp.gate_proj\n",
      "layers.14.mlp.up_proj\n",
      "layers.14.mlp.down_proj\n",
      "layers.14.mlp.act_fn\n",
      "layers.14.input_layernorm\n",
      "layers.14.post_attention_layernorm\n",
      "layers.15\n",
      "layers.15.self_attn\n",
      "layers.15.self_attn.q_proj\n",
      "layers.15.self_attn.k_proj\n",
      "layers.15.self_attn.v_proj\n",
      "layers.15.self_attn.o_proj\n",
      "layers.15.self_attn.rotary_emb\n",
      "layers.15.mlp\n",
      "layers.15.mlp.gate_proj\n",
      "layers.15.mlp.up_proj\n",
      "layers.15.mlp.down_proj\n",
      "layers.15.mlp.act_fn\n",
      "layers.15.input_layernorm\n",
      "layers.15.post_attention_layernorm\n",
      "layers.16\n",
      "layers.16.self_attn\n",
      "layers.16.self_attn.q_proj\n",
      "layers.16.self_attn.k_proj\n",
      "layers.16.self_attn.v_proj\n",
      "layers.16.self_attn.o_proj\n",
      "layers.16.self_attn.rotary_emb\n",
      "layers.16.mlp\n",
      "layers.16.mlp.gate_proj\n",
      "layers.16.mlp.up_proj\n",
      "layers.16.mlp.down_proj\n",
      "layers.16.mlp.act_fn\n",
      "layers.16.input_layernorm\n",
      "layers.16.post_attention_layernorm\n",
      "layers.17\n",
      "layers.17.self_attn\n",
      "layers.17.self_attn.q_proj\n",
      "layers.17.self_attn.k_proj\n",
      "layers.17.self_attn.v_proj\n",
      "layers.17.self_attn.o_proj\n",
      "layers.17.self_attn.rotary_emb\n",
      "layers.17.mlp\n",
      "layers.17.mlp.gate_proj\n",
      "layers.17.mlp.up_proj\n",
      "layers.17.mlp.down_proj\n",
      "layers.17.mlp.act_fn\n",
      "layers.17.input_layernorm\n",
      "layers.17.post_attention_layernorm\n",
      "layers.18\n",
      "layers.18.self_attn\n",
      "layers.18.self_attn.q_proj\n",
      "layers.18.self_attn.k_proj\n",
      "layers.18.self_attn.v_proj\n",
      "layers.18.self_attn.o_proj\n",
      "layers.18.self_attn.rotary_emb\n",
      "layers.18.mlp\n",
      "layers.18.mlp.gate_proj\n",
      "layers.18.mlp.up_proj\n",
      "layers.18.mlp.down_proj\n",
      "layers.18.mlp.act_fn\n",
      "layers.18.input_layernorm\n",
      "layers.18.post_attention_layernorm\n",
      "layers.19\n",
      "layers.19.self_attn\n",
      "layers.19.self_attn.q_proj\n",
      "layers.19.self_attn.k_proj\n",
      "layers.19.self_attn.v_proj\n",
      "layers.19.self_attn.o_proj\n",
      "layers.19.self_attn.rotary_emb\n",
      "layers.19.mlp\n",
      "layers.19.mlp.gate_proj\n",
      "layers.19.mlp.up_proj\n",
      "layers.19.mlp.down_proj\n",
      "layers.19.mlp.act_fn\n",
      "layers.19.input_layernorm\n",
      "layers.19.post_attention_layernorm\n",
      "layers.20\n",
      "layers.20.self_attn\n",
      "layers.20.self_attn.q_proj\n",
      "layers.20.self_attn.k_proj\n",
      "layers.20.self_attn.v_proj\n",
      "layers.20.self_attn.o_proj\n",
      "layers.20.self_attn.rotary_emb\n",
      "layers.20.mlp\n",
      "layers.20.mlp.gate_proj\n",
      "layers.20.mlp.up_proj\n",
      "layers.20.mlp.down_proj\n",
      "layers.20.mlp.act_fn\n",
      "layers.20.input_layernorm\n",
      "layers.20.post_attention_layernorm\n",
      "layers.21\n",
      "layers.21.self_attn\n",
      "layers.21.self_attn.q_proj\n",
      "layers.21.self_attn.k_proj\n",
      "layers.21.self_attn.v_proj\n",
      "layers.21.self_attn.o_proj\n",
      "layers.21.self_attn.rotary_emb\n",
      "layers.21.mlp\n",
      "layers.21.mlp.gate_proj\n",
      "layers.21.mlp.up_proj\n",
      "layers.21.mlp.down_proj\n",
      "layers.21.mlp.act_fn\n",
      "layers.21.input_layernorm\n",
      "layers.21.post_attention_layernorm\n",
      "layers.22\n",
      "layers.22.self_attn\n",
      "layers.22.self_attn.q_proj\n",
      "layers.22.self_attn.k_proj\n",
      "layers.22.self_attn.v_proj\n",
      "layers.22.self_attn.o_proj\n",
      "layers.22.self_attn.rotary_emb\n",
      "layers.22.mlp\n",
      "layers.22.mlp.gate_proj\n",
      "layers.22.mlp.up_proj\n",
      "layers.22.mlp.down_proj\n",
      "layers.22.mlp.act_fn\n",
      "layers.22.input_layernorm\n",
      "layers.22.post_attention_layernorm\n",
      "layers.23\n",
      "layers.23.self_attn\n",
      "layers.23.self_attn.q_proj\n",
      "layers.23.self_attn.k_proj\n",
      "layers.23.self_attn.v_proj\n",
      "layers.23.self_attn.o_proj\n",
      "layers.23.self_attn.rotary_emb\n",
      "layers.23.mlp\n",
      "layers.23.mlp.gate_proj\n",
      "layers.23.mlp.up_proj\n",
      "layers.23.mlp.down_proj\n",
      "layers.23.mlp.act_fn\n",
      "layers.23.input_layernorm\n",
      "layers.23.post_attention_layernorm\n",
      "layers.24\n",
      "layers.24.self_attn\n",
      "layers.24.self_attn.q_proj\n",
      "layers.24.self_attn.k_proj\n",
      "layers.24.self_attn.v_proj\n",
      "layers.24.self_attn.o_proj\n",
      "layers.24.self_attn.rotary_emb\n",
      "layers.24.mlp\n",
      "layers.24.mlp.gate_proj\n",
      "layers.24.mlp.up_proj\n",
      "layers.24.mlp.down_proj\n",
      "layers.24.mlp.act_fn\n",
      "layers.24.input_layernorm\n",
      "layers.24.post_attention_layernorm\n",
      "layers.25\n",
      "layers.25.self_attn\n",
      "layers.25.self_attn.q_proj\n",
      "layers.25.self_attn.k_proj\n",
      "layers.25.self_attn.v_proj\n",
      "layers.25.self_attn.o_proj\n",
      "layers.25.self_attn.rotary_emb\n",
      "layers.25.mlp\n",
      "layers.25.mlp.gate_proj\n",
      "layers.25.mlp.up_proj\n",
      "layers.25.mlp.down_proj\n",
      "layers.25.mlp.act_fn\n",
      "layers.25.input_layernorm\n",
      "layers.25.post_attention_layernorm\n",
      "layers.26\n",
      "layers.26.self_attn\n",
      "layers.26.self_attn.q_proj\n",
      "layers.26.self_attn.k_proj\n",
      "layers.26.self_attn.v_proj\n",
      "layers.26.self_attn.o_proj\n",
      "layers.26.self_attn.rotary_emb\n",
      "layers.26.mlp\n",
      "layers.26.mlp.gate_proj\n",
      "layers.26.mlp.up_proj\n",
      "layers.26.mlp.down_proj\n",
      "layers.26.mlp.act_fn\n",
      "layers.26.input_layernorm\n",
      "layers.26.post_attention_layernorm\n",
      "layers.27\n",
      "layers.27.self_attn\n",
      "layers.27.self_attn.q_proj\n",
      "layers.27.self_attn.k_proj\n",
      "layers.27.self_attn.v_proj\n",
      "layers.27.self_attn.o_proj\n",
      "layers.27.self_attn.rotary_emb\n",
      "layers.27.mlp\n",
      "layers.27.mlp.gate_proj\n",
      "layers.27.mlp.up_proj\n",
      "layers.27.mlp.down_proj\n",
      "layers.27.mlp.act_fn\n",
      "layers.27.input_layernorm\n",
      "layers.27.post_attention_layernorm\n",
      "layers.28\n",
      "layers.28.self_attn\n",
      "layers.28.self_attn.q_proj\n",
      "layers.28.self_attn.k_proj\n",
      "layers.28.self_attn.v_proj\n",
      "layers.28.self_attn.o_proj\n",
      "layers.28.self_attn.rotary_emb\n",
      "layers.28.mlp\n",
      "layers.28.mlp.gate_proj\n",
      "layers.28.mlp.up_proj\n",
      "layers.28.mlp.down_proj\n",
      "layers.28.mlp.act_fn\n",
      "layers.28.input_layernorm\n",
      "layers.28.post_attention_layernorm\n",
      "layers.29\n",
      "layers.29.self_attn\n",
      "layers.29.self_attn.q_proj\n",
      "layers.29.self_attn.k_proj\n",
      "layers.29.self_attn.v_proj\n",
      "layers.29.self_attn.o_proj\n",
      "layers.29.self_attn.rotary_emb\n",
      "layers.29.mlp\n",
      "layers.29.mlp.gate_proj\n",
      "layers.29.mlp.up_proj\n",
      "layers.29.mlp.down_proj\n",
      "layers.29.mlp.act_fn\n",
      "layers.29.input_layernorm\n",
      "layers.29.post_attention_layernorm\n",
      "layers.30\n",
      "layers.30.self_attn\n",
      "layers.30.self_attn.q_proj\n",
      "layers.30.self_attn.k_proj\n",
      "layers.30.self_attn.v_proj\n",
      "layers.30.self_attn.o_proj\n",
      "layers.30.self_attn.rotary_emb\n",
      "layers.30.mlp\n",
      "layers.30.mlp.gate_proj\n",
      "layers.30.mlp.up_proj\n",
      "layers.30.mlp.down_proj\n",
      "layers.30.mlp.act_fn\n",
      "layers.30.input_layernorm\n",
      "layers.30.post_attention_layernorm\n",
      "layers.31\n",
      "layers.31.self_attn\n",
      "layers.31.self_attn.q_proj\n",
      "layers.31.self_attn.k_proj\n",
      "layers.31.self_attn.v_proj\n",
      "layers.31.self_attn.o_proj\n",
      "layers.31.self_attn.rotary_emb\n",
      "layers.31.mlp\n",
      "layers.31.mlp.gate_proj\n",
      "layers.31.mlp.up_proj\n",
      "layers.31.mlp.down_proj\n",
      "layers.31.mlp.act_fn\n",
      "layers.31.input_layernorm\n",
      "layers.31.post_attention_layernorm\n",
      "norm\n"
     ]
    }
   ],
   "source": [
    "for nm in model.named_modules(): print(nm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "472fd948-aa24-4403-83da-81c8ce14be6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b93978b85f41b997bdbef5e278c15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(model_path, load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c83294ca-3f67-44ff-bda7-31cd8bb03baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 131338240 || all params: 3620999168 || trainable%: 3.63\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba1eecd-80b7-428a-b6f6-290e6cc735ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf3c3086-5cbd-422f-a59e-1208502be8de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85a7f2da-7b21-4cb5-ab34-257e7b9e6f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 16, \n",
    "    lora_alpha= 16, \n",
    "    target_modules= [\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=.1, \n",
    "    bias=\"none\"\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64099d0-3fc8-4796-8046-ed592fe3cdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f78c498-216d-4748-913a-ef47d4bd2ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora__model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef85edd7-9a63-4a54-9dc7-a5412a458e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7b60801-f120-41c2-a072-ac6cc3b832b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b53256f4-713b-477e-ae51-4ef730110c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"perfModel\", \n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\", \n",
    "    learning_rate=5e-3, \n",
    "    num_train_epochs=5, \n",
    "    per_device_eval_batch_size=batch_size, \n",
    "    gradient_accumulation_steps=2, \n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ffccdf-0b11-4c5b-a519-eb3d0bfb8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    lora_model, \n",
    "    args, \n",
    "    train_dataset=\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
