{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ae0273-6d0c-4f40-8b37-f93b173f69b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939e1cfa-2ef8-4515-a7ff-df8963d24938",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = { \"load_in_4bit\" : True  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f5fb38-21d3-4715-9f41-d371e45fe246",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41418d273d34e0e86e447f46d0e9f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.2\", max_new_tokens=1000, return_full_text=False, model_kwargs= model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28d75cf-a476-4540-9ef3-1f8877ac8206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = \"LlamaIndex is a data framework for LLM-based applications which benefit from context augmentation. Such LLM systems have been termed as RAG systems, standing for “Retrieval-Augemented Generation”. .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04260115-26ef-497a-ad3b-9d431b757369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"testdoc2.txt\", 'r') as file: \n",
    "    t2 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a3920f8-8c78-418f-9c16-41f17e31a911",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2544bd58-4f1c-43ae-a153-63df6cf96f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t2_prompt_unnumbered = \"[INST]You are an expert user extracting information to quiz people on documentation. You will be passed a page extracted from the documentation.  Write an unnumbered list of 10 questions that can be answered based *solely* on the given text, and include the answer under each question.  Prefix the question with the word question followed by a colon, and prefix the answer with the word answer followed by a colon\"+ t2+ \"[/INST]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a02764a-710a-4019-9a04-8c2ca1ab60a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_numbered = \"You are an expert user extracting information to quiz people on documentation. You will be passed a page extracted from the documentation.  Write a numbered list of 3 questions that can be answered based *solely* on the given text, and include the answer under each question.  Prefix the question with the word question followed by a colon, and prefix the answer with the word answer followed by a colon\"+ t1+ \"[/INST]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03dfbeda-6468-4965-9559-c179f95b3266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_unnumbered2 = \"You are an expert user extracting information to quiz people on documentation. You will be passed a page extracted from the documentation.  Write an unnumbered list of 2 questions that can be answered based *solely* on the given text, and include the answer under each question.  \"+ t1+ \"[/INST]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd81f504-15a3-4290-8332-4e71eeb5b1c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_unnumbered = \"[INST]You are an expert user extracting information to quiz people on documentation. You will be passed a page extracted from the documentation.  Write an unnumbered list of 2 questions that can be answered based *solely* on the given text, and include the answer under each question.  Prefix the question with the word question followed by a colon, and prefix the answer with the word answer followed by a colon\"+ t1+ \"[/INST]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05eae2af-4ce2-4aa8-8b41-094f1d17a6f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. Question: What is LlamaIndex and what type of systems does it benefit?\n",
      "Answer: LlamaIndex is a data framework for Retrieval-Augmented Generation (RAG) systems, which are LLM-based applications that benefit from context augmentation.\n",
      "\n",
      "2. Question: What are RAG systems and how do they differ from fine-tuned LLMs?\n",
      "Answer: RAG systems are a context augmentation pattern that retrieves information from data sources, adds it to a question as context, and asks the LLM to answer based on the enriched prompt. They differ from fine-tuned LLMs in that they don't require training, are always up to date, and offer more trustworthiness.\n",
      "\n",
      "3. Question: What are the benefits of using LlamaIndex for context augmentation?\n",
      "Answer: LlamaIndex imposes no restriction on how LLMs are used, provides tools to quickly stand up production-ready RAG systems, and offers data connectors, data indexes, and engines for natural language access to data.\n",
      "\n",
      "4. Question: What are the high-level steps involved in RAG?\n",
      "Answer: The high-level steps involved in RAG are retrieving information from data sources, adding it to the question as context, and asking the LLM to answer based on the enriched prompt.\n",
      "\n",
      "5. Question: What are the advantages of using RAG over fine-tuning a LLM?\n",
      "Answer: RAG overcomes the weaknesses of fine-tuning by being cheaper, always up to date, and more trustworthy.\n",
      "\n",
      "6. Question: What type of data sources can LlamaIndex ingest?\n",
      "Answer: LlamaIndex can ingest data from various sources such as APIs, SQL databases, and PDFs, among others.\n",
      "\n",
      "7. Question: What tools does LlamaIndex provide to help users quickly stand up production-ready RAG systems?\n",
      "Answer: LlamaIndex provides data connectors, data indexes, and engines to help users quickly stand up production-ready RAG systems.\n",
      "\n",
      "8. Question: What are the different types of engines provided by LlamaIndex?\n",
      "Answer: LlamaIndex provides query engines and chat engines to provide natural language access to data.\n",
      "\n",
      "9. Question: Who is LlamaIndex for and what level of users does it cater to?\n",
      "Answer: LlamaIndex caters to users of all levels, from beginners to advanced users, and offers a high-level API for easy use and lower-level APIs for advanced customization.\n",
      "\n",
      "10. Question: What are the use cases for LlamaIndex?\n",
      "Answer: LlamaIndex can be used for various applications such as auto-complete, chatbots, semi-autonomous agents, and more.\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(t2_prompt_unnumbered, max_new_tokens=1000, do_sample=True, temperature=0.99, top_k=100, top_p=0.1)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2460f890-5e14-40c2-a6ad-0e295038d9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' Question: What type of systems does LlamaIndex cater to?\\nAnswer: LlamaIndex is a data framework for LLM-based systems.\\n\\nQuestion: What is the acronym RAG used to represent in the context of LlamaIndex?\\nAnswer: RAG stands for Retrieval-Augmented Generation.'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94a7b5df-3ce3-46c7-87ea-28da4935c114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = outputs[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee542d75-e200-47df-975d-d9de4b494ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" 1. Question: What is LlamaIndex and what type of systems does it benefit?\\nAnswer: LlamaIndex is a data framework for Retrieval-Augmented Generation (RAG) systems, which are LLM-based applications that benefit from context augmentation.\\n\\n2. Question: What are RAG systems and how do they differ from fine-tuned LLMs?\\nAnswer: RAG systems are a context augmentation pattern that retrieves information from data sources, adds it to a question as context, and asks the LLM to answer based on the enriched prompt. They differ from fine-tuned LLMs in that they don't require training, are always up to date, and offer more trustworthiness.\\n\\n3. Question: What are the benefits of using LlamaIndex for context augmentation?\\nAnswer: LlamaIndex imposes no restriction on how LLMs are used, provides tools to quickly stand up production-ready RAG systems, and offers data connectors, data indexes, and engines for natural language access to data.\\n\\n4. Question: What are the high-level steps involved in RAG?\\nAnswer: The high-level steps involved in RAG are retrieving information from data sources, adding it to the question as context, and asking the LLM to answer based on the enriched prompt.\\n\\n5. Question: What are the advantages of using RAG over fine-tuning a LLM?\\nAnswer: RAG overcomes the weaknesses of fine-tuning by being cheaper, always up to date, and more trustworthy.\\n\\n6. Question: What type of data sources can LlamaIndex ingest?\\nAnswer: LlamaIndex can ingest data from various sources such as APIs, SQL databases, and PDFs, among others.\\n\\n7. Question: What tools does LlamaIndex provide to help users quickly stand up production-ready RAG systems?\\nAnswer: LlamaIndex provides data connectors, data indexes, and engines to help users quickly stand up production-ready RAG systems.\\n\\n8. Question: What are the different types of engines provided by LlamaIndex?\\nAnswer: LlamaIndex provides query engines and chat engines to provide natural language access to data.\\n\\n9. Question: Who is LlamaIndex for and what level of users does it cater to?\\nAnswer: LlamaIndex caters to users of all levels, from beginners to advanced users, and offers a high-level API for easy use and lower-level APIs for advanced customization.\\n\\n10. Question: What are the use cases for LlamaIndex?\\nAnswer: LlamaIndex can be used for various applications such as auto-complete, chatbots, semi-autonomous agents, and more.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb0ee590-f8f1-4374-a8e5-2171777badb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_answers = answer.split(\"\\n\\nQuestion: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71d4b2d4-7ed3-4a88-ad4e-30d1ea663d92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_answers0 = answer.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e8595f6-78a4-4381-8edb-9d7522c752c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" 1. Question: What is LlamaIndex and what type of systems does it benefit?\\nAnswer: LlamaIndex is a data framework for Retrieval-Augmented Generation (RAG) systems, which are LLM-based applications that benefit from context augmentation.\\n\\n2. Question: What are RAG systems and how do they differ from fine-tuned LLMs?\\nAnswer: RAG systems are a context augmentation pattern that retrieves information from data sources, adds it to a question as context, and asks the LLM to answer based on the enriched prompt. They differ from fine-tuned LLMs in that they don't require training, are always up to date, and offer more trustworthiness.\\n\\n3. Question: What are the benefits of using LlamaIndex for context augmentation?\\nAnswer: LlamaIndex imposes no restriction on how LLMs are used, provides tools to quickly stand up production-ready RAG systems, and offers data connectors, data indexes, and engines for natural language access to data.\\n\\n4. Question: What are the high-level steps involved in RAG?\\nAnswer: The high-level steps involved in RAG are retrieving information from data sources, adding it to the question as context, and asking the LLM to answer based on the enriched prompt.\\n\\n5. Question: What are the advantages of using RAG over fine-tuning a LLM?\\nAnswer: RAG overcomes the weaknesses of fine-tuning by being cheaper, always up to date, and more trustworthy.\\n\\n6. Question: What type of data sources can LlamaIndex ingest?\\nAnswer: LlamaIndex can ingest data from various sources such as APIs, SQL databases, and PDFs, among others.\\n\\n7. Question: What tools does LlamaIndex provide to help users quickly stand up production-ready RAG systems?\\nAnswer: LlamaIndex provides data connectors, data indexes, and engines to help users quickly stand up production-ready RAG systems.\\n\\n8. Question: What are the different types of engines provided by LlamaIndex?\\nAnswer: LlamaIndex provides query engines and chat engines to provide natural language access to data.\\n\\n9. Question: Who is LlamaIndex for and what level of users does it cater to?\\nAnswer: LlamaIndex caters to users of all levels, from beginners to advanced users, and offers a high-level API for easy use and lower-level APIs for advanced customization.\\n\\n10. Question: What are the use cases for LlamaIndex?\\nAnswer: LlamaIndex can be used for various applications such as auto-complete, chatbots, semi-autonomous agents, and more.\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ef20e4e-09ad-4b4a-9cc3-aa3cfc2e6cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question_answer \u001b[38;5;129;01min\u001b[39;00m question_answers: \n\u001b[0;32m----> 2\u001b[0m     question, answer \u001b[38;5;241m=\u001b[39m question_answer\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m question\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Question: \u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      4\u001b[0m         question \u001b[38;5;241m=\u001b[39m question\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Question: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for question_answer in question_answers: \n",
    "    question, answer = question_answer.split(\"\\nAnswer: \")\n",
    "    if question.startswith(' Question: '):\n",
    "        question = question.replace(\" Question: \", \"\")\n",
    "    print(question)\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b09bf-e61a-47cf-b5a2-914fdb6373ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
